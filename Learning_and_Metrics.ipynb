{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning\n",
    "\n",
    "For this and all other machine-learning setups, \n",
    "\n",
    "$$X = \\{X_{ji}\\}$$\n",
    "\n",
    "is an $n \\times p$ matrix of features and \n",
    "\n",
    "$$y_j$$\n",
    "\n",
    "is an $n$-vector of labels.  In all **supervised** learning problems, we are trying to build a **model** $f$ (predictive relationship) that maps the feature rows $\\{X_{j \\cdot}\\}$ to each label $y_j$ so that \n",
    "\n",
    "$$f(X_{j \\cdot}) \\approx y_j.$$\n",
    "\n",
    "With this predictive model, future we will be able to predict the label associated with new feature row $X'_{j \\cdot}$ via $f(X'_{j \\cdot})$.  That's it.\n",
    "\n",
    "A few key concepts:\n",
    "1. Within Supervised machine learning, there are roughly two broad classes of problems, **Regression** and **Classification**.  Regression is when the values of $y$ are continuous and real-valued.  Classification is when $y$ takes on a discrete discrete number of possible values.\n",
    "1. What does it mean for the prediction to be accurate?  That depends on what **metric** we use, which is at the discretion of the modeller.  Obviously, some metrics make sense for regression, and others for classification.  And some make sense for \n",
    "\n",
    "**Question:** \n",
    "1. What are some examples of classification versus regression problems or algorithms?\n",
    "1. These are all examples of supervised learning problems or algorithms.  Do you know of an example of an unsupervised learning problem or algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Regression\n",
    "\n",
    "**Sum of Squared Error** is the usual metric:\n",
    "\n",
    "$$ \\sum_j \\left|f(X_{j \\cdot}) - y_j\\right|^2. $$\n",
    "\n",
    "Unforunately, this is susceptible to outliers. When this is an assue, **Absolute Error** can be better:\n",
    "\n",
    "$$ \\sum_j \\left|f(X_{j \\cdot}) - y_j\\right|. $$\n",
    "\n",
    "You've probably heard of **$R^2$** or the **Coefficient of Determination**. Although it's usually defined in a linear regression context, it's actually a very general idea: it measure the fraction of the error explained by the model $f$ versus the fraction of the error explained by a naive model that assumes the mean value of $y$ (i.e. the variance of $y$):\n",
    "\n",
    "$$ 1 - \\dfrac{\\sum_j \\left|f(X_{j \\cdot}) - y_j\\right|^2}{\\sum_j \\left|\\overline y - y_j\\right|^2} \\qquad \\mbox{where} \\qquad \\overline y = \\frac{1}{n}\\sum_j y_j \\,.$$\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "1. For a list of scalar values $z_1,\\ldots,z_n$, the **mean** $\\overline z$ is the quantity that minimizes the squared error:\n",
    "$$ \\frac{d}{dz} \\sum_j \\left|z - z_j\\right|^2 = 0$$\n",
    "$$ \\frac{d}{dz} \\left(Nz^2 - 2z(z_1 + z_2 + ...) + z_1^2 + z_2^2 + ...\\right) = 0$$\n",
    "$$ 2Nz - 2(z_1 + z_2 + z_3 + ...) = 0$$\n",
    "\n",
    "$$ z = \\frac{z_1 + z_2 + z_3 + ...}{N} = \\overline z$$\n",
    "\n",
    "Do you know what quantity comes from minimizing the absolute error?\n",
    "$$ \\mbox{argmin}_z \\sum_j \\left|z - z_j\\right| $$\n",
    "Does this help explain why Absolute Error is less susceptible to outliers?\n",
    "1. How does each of these metrics scale as you scale the labels ($y$'s) in our data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here are those metrics in scikit learn\n",
    "\n",
    "from sklearn import metrics\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "y_obs  = 2*random.randn(10)\n",
    "y_pred = y_obs + .1 * random.randn(10)\n",
    "\n",
    "print \"Mean Absolute Error:\", metrics.mean_absolute_error(y_obs, y_pred)\n",
    "print \"Mean Squared Error:\", metrics.mean_squared_error(y_obs, y_pred)\n",
    "print \"R^2:\", metrics.r2_score(y_obs, y_pred)\n",
    "\n",
    "# Plot mean-squared error\n",
    "X = np.arange(-3,3,.1)\n",
    "plt.plot(X,abs(X),label='absolute_error')\n",
    "plt.plot(X, X**2,label='squared_error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Classificaton\n",
    "\n",
    "There are a plethora of metrics for classification and they depend on whether the predictions are given in terms of the potential label classes or probabilities.\n",
    "\n",
    "## Metrics for Class Predictions\n",
    "\n",
    "Let's start with the simplest.\n",
    "\n",
    "Recall this well-known table\n",
    "\n",
    "|                     | Observation Positive     | Observation Negative    |\n",
    "|---------------------|:------------------------:|:-----------------------:|\n",
    "| Prediction Positive |     True Positive        | False Positive (Type I) |\n",
    "| Prediction Negative | False Negative (Type II) |     True Negative       |\n",
    "\n",
    "There are many summary statistics one can compute from this table:\n",
    "1. The **Accuracy** gives the fraction labels correctly predicted (True Positives and True Negatives over everything).  \n",
    "1. The **Hamming Loss** gives the fraction of labels incorrectly predicted.  It is 1 - Accuracy.\n",
    "1. The **Precision** is true positives divided by all positive *predictions*\n",
    "1. The **Recall** is true positives divided by all positive *observations*.\n",
    "1. There is also **F-beta** score which gives a weighted geometric average between the precision and recall (as a function of $\\beta$) and the **F-1** score is the special case when $\\beta = 1$.\n",
    "1. The **Jaccard Similarity Coefficient** is in general the intersection of the predicted and actual label set divided by the union. This is equivalent to the accuracy score for most classification problems.\n",
    "\n",
    "**Question:**\n",
    "1. What's the interpretation of precision or recall?  When would you want each?  Is Harvard's admission's process high precision or high recall?  What about Sir Blackstone's aphorism \"Better that ten guilty persons escape than that one innocent suffer\" with Captain Louis Renault's order to \"Round up the Usual Suspects\" in the film \"Casablanca\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy and Hamming distnace:\n",
    "\n",
    "y_obs  = [0, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [0, 0, 1, 1, 0, 0, 0, 1]\n",
    "\n",
    "print \"Accuracy:\", metrics.accuracy_score(y_obs, y_pred)\n",
    "print \"Hamming Loss:\", metrics.hamming_loss(y_obs, y_pred)\n",
    "print \"Precision:\", metrics.precision_score(y_obs, y_pred)\n",
    "print \"Recall:\", metrics.recall_score(y_obs, y_pred)\n",
    "print \"F1:\", metrics.f1_score(y_obs, y_pred)\n",
    "print \"Jaccard:\", metrics.jaccard_similarity_score(y_obs, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for probabilistic predictions\n",
    "\n",
    "### Precision-Recall Tradeoff\n",
    "\n",
    "When the predictions are in terms of probabilities, we have a different set of metrics.  Then the above metrics are not well defined.  However, there is an implicit tradeoff between the two.\n",
    "\n",
    "Let us take the two-class example where each $y_j$ is either positive (1) or negative (0) and we make a probabilistic prediction $p_j$.  A reasonable solution would be to choose a **threshold** $\\underline p$ such that $p_j > \\underline p$ is a Positive Prediciton and $p_j \\le \\underline p$ is a negative one.  Hence, for every choice of $p_j$, we can compute a precision and a recall.  Varying $\\underline p$ creates a family (or curve) of Precision Recall pairs.\n",
    "\n",
    "**Questions:** \n",
    "1. How does increasing $\\underline p$ affect the Precision or Recall?  What assumption do you have to make in order to answer this?\n",
    "1. How does the curve vary depending on the quality of the estimator $f$ that produces the predictions $p_j$.  What if the estimator were perfect?  What if it were guessing at random?\n",
    "1. What if $f$ were a reasonably good estimator but you used $1-f$ as your estimator?\n",
    "1. How do you decide how to make the tradeoff between precision and recall?  How does this relate to the cost of a false positive versus false negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Precision-Recall tradeoff\n",
    "\n",
    "# generate predictions and observations\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "y_pred = random.beta(1,1, size=1000)\n",
    "y_obs  = np.array([random.binomial(1, p) for p in y_pred])  # Can anyone \"vectorize\" this?\n",
    "\n",
    "# compute the relevant stats\n",
    "precisions, recalls, thresholds = metrics.precision_recall_curve(y_obs, y_pred)\n",
    "thresholds = np.hstack([[0.], thresholds])  # n precisions but n-1 thresholds\n",
    "f1s = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "\n",
    "# plots\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(thresholds, precisions, label='Precision')\n",
    "plt.plot(thresholds, recalls, label='Recall')\n",
    "plt.plot(thresholds, f1s, label='F1 Score')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel(\"Threshold\"); plt.ylabel(\" \")\n",
    "plt.title(\"Precision, Recall, and F1 Score vs Thresholds\")\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.plot(recalls, precisions)\n",
    "plt.xlim([0., 1.]); plt.ylim([0., 1.])\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs Recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have a precision and recall tradeoff for probabilistic estimators, we usually report statistics like \"Precision at .6\", which means \"Precision when the threshold is set at Recall=.6\" and vice versa for \"Recall at .8\".\n",
    "\n",
    "**Question**:\n",
    "1. There's a tradeoff between Precision and Recall.  When would you want a high precision process?  When would you want a high recall process?  Which would you use for Email Spam detection?  What about Drug approvals?\n",
    "1. Let's suppose the NSA has an estimator for \"likely to be a terrorist\" which they use to determine who should be surveilled.  How do enhanced national security versus fourth-amendment protections map onto precsion and recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-valued Probabilistic Prediction Metrics\n",
    "\n",
    "While a modeller can decide the appropriate threshold once given a precision-recall curve, it is hard to optimzie for and it doesn't necessarily make sense to optimize for \"Precision at .6\" (why not \"Precision at .7\"?).  We need a single-valued metric that is independent of threshold.  Fortunately, there are two common ones:\n",
    "\n",
    "1. The **Area under the Curve** or **AUC** computes the area under the Precision Recall curve.\n",
    "1. There is a **Receiver Operating Charateristic**, which is similar to the Precision-Recall curve.  The area under this curve is itself a metric called **ROC-AUC**.  The definition isn't hard, but it's beyond the scope of this course.  You can find out more [on Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) or develop some geometric intuition from this [blog post](https://shapeofdata.wordpress.com/2015/01/05/precision-recall-aucs-and-rocs/).\n",
    "1. The **Log-Loss** or **Entropy** is another characteristic.  It is related to the notion of Entropy in Thermodynamics and Shannon-Entropy.  For a binary class problem where $y_j$ is either $0$ or $1$, it is given by\n",
    "\n",
    "$$ - \\sum_j \\left[y_j \\log(p_j) + (1-y_j) \\log(1-p_j)\\right] $$\n",
    "\n",
    "**Question:** \n",
    "1. Can you generalize entropy formula from a two-class metric to an $m$-class metric?  What about for the other metrics?\n",
    "1. In Scikit Learn, `metrics.auc` computes AUC form the precision and recall numbers while `metrics.average_precision_score` computes AUC directly from observations and predictions.  Why is average precision the same as AUC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AUC, ROC-AUC, Log-loss\n",
    "\n",
    "# for entropy, we need the predictions and observations by-class\n",
    "# e.g. two columns for a binomial prediction.\n",
    "y_pred_full = np.vstack([y_pred, 1-y_pred]).T\n",
    "y_obs_full = np.vstack([y_obs, 1-y_obs]).T\n",
    "\n",
    "# You can compute AUC either via the AUC function by the average_precision_score\n",
    "print \"AUC by hand:\", metrics.auc(recalls, precisions)\n",
    "print \"AUC:\", metrics.average_precision_score(y_obs, y_pred)\n",
    "print \"ROC-AUC:\", metrics.roc_auc_score(y_obs, y_pred)\n",
    "print \"Entropy:\", metrics.log_loss(y_obs_full, y_pred_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Which of these metrics do you want to increase, and which do you want to decrease?  Hint: in `sklearn`, metrics you wish to increase usually end in `*_score` while metrics you wish to decrease usually end in `*_error`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "Another common metric we usually want to understand is how important a specific feature or signal is.\n",
    "1.  One common general technique is to leave a column out, retrain the model, and measure the drop in the metric.  This can be expensive because it requires retraining a model.\n",
    "1.  Another common technique is to use the same model but feed in data with one column randomly shuffled.  Measuring the loss on the performance metric compared with feeding in the non-shuffled data is another performance metric.\n",
    "\n",
    "**Question:**\n",
    "1. How would you do this for a linear model?\n",
    "1. Why is leave one out analysis computationally expensive?\n",
    "1. When using leave a column out, what happens if two columns are either identical or nearly identical alises for one another?\n",
    "1. How does randomly shuffling data in a column affect the performance of the predictions?\n",
    "1. What are some other strategies that might counter some of the shortcomings of the above techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric versus Parametric Analysis\n",
    "\n",
    "One of the key distinctions in stats is between parametric and non-parametric statistics.  Rather than making anything specific distributional assumptions (e.g. in Hypothesis testing, we assume that we're seeing normally distributed data) or model form assumptions (e.g. in Linear Regression, we are assuming linear relationship between the dependent and independent variables), we do statistics without making those assumptions.  For examle, we can use non-parametric statistics techniques like [Kernel Density Estimation](http://en.wikipedia.org/wiki/Kernel_density_estimation) to compare two dsitributions or use [Gaussian Process Regression](http://en.wikipedia.org/wiki/Gaussian_process_regression) (Scikit documentation [available here](http://scikit-learn.org/stable/modules/gaussian_process.html)) to perform regression analysis.  We won't talk about non-parametric statistics too much here but it's a good thing to know this distrinction exists.\n",
    "\n",
    "Usuaully, by putting in parametric assumptions, we find it easier to learn about our data by introducing a prior (in the Bayesian sense) into our modelling assumptions.  However, if this prior is incorrect (e.g. the data isn't well-explained by a linear relationship), then we could be going astray.\n",
    "\n",
    "## Asymptotic Approximation Property\n",
    "\n",
    "Some models, e.g. Linear Regression, have floor for how much better they are able to fit the data, even with larger amounts of it.  If the underlying model is not linear, then no amount of data will make the model fit the data better.  In general, decision trees, boosting algorithms, and neural networks do satisfy this property.  What these models gain in precision, they often loose in explanability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exit Tickets\n",
    "1. How does the choice of error metric influence the effect of outliers in the data?\n",
    "1. Describe to your grandma what precision and recall are, and the precision-recall tradeoff.\n",
    "1. What metrics can I use for probabilistic predictions if I don't want to use a threshold?\n",
    "1. At what stage of a data science project should you be thinking about feature importance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoilers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers\n",
    "\n",
    "### Metrics for Regression\n",
    "\n",
    "1. Minimizing the absolute error finds the median.  Consider the \"derivative\" of $$\\left|z - z_j\\right| $$ with respect to $z$.  The median is less suceptible to outliers.  This gives some explanation of why (outliers have a linear, rather than quadratic effect).\n",
    "1. $R^2$ is invariant to $y$, Absolute Error linearly, and MSE quadratically.\n",
    "\n",
    "### Metrics for Class Predictions\n",
    "1. Precision gives what fraction of positive predictions we got right.  Recall tells us how many true positives we were able to find.\n",
    "1. Perhaps controvertial but I would say Harvard is a high precision but not high recall.  A large state school might be higher recall but lower precision.\n",
    "1. Blackstone argued for a high precision process in judicial proceedings, at the expense of recall.  Renault wants a high recall process, perhaps at the expense of precision.\n",
    "\n",
    "### Precision-Recall Tradeoff\n",
    "1. Increasing $\\underline p$ increases precision and decreases recall.  Here, we are assuming that the estimator is halfway decent.\n",
    "1. A perfect estimator would be a horizontal line across the top of the graph.  If it were guessing at random, it would be a horizontal line at the global rate of positive examples (precision at recall = 1.).\n",
    "1. Using $1-f$ will result in an estimator that is below the line of a random estimator (i.e. it is strictly worse than just guessing).\n",
    "1. We want a high precision process when the cost of a false positive is high.  We want a high recall process when the cost of a false negative is high.\n",
    "1. You want a high precision Spam Detector (better to read a few extra spam emails then that email telling you about your promotion).\n",
    "1. FDA Drug Approvals are very risk averse: they are looking for a high precision process.\n",
    "1. Optimizing for enhanced national security, you want high recall (false negatives are terrorist attacks).  Optimizing for civil liberties, you want high precision (false positives are essentially violations of civil liberties).\n",
    "\n",
    "### Single-valued Probabilistic Prediction Metrics\n",
    "1.  If $p_{jk}$ is the probability that example $j$ will be in class $k$, then the entropy is\n",
    "    $$ \\sum_j \\sum_{k: y_j = k}\\log(p_{jk}) \\,.$$\n",
    "    Remember that $\\sum_k p_{jk} = 1$.\n",
    "1.  The average precision means precision integrated over recall.  If you draw out the picture, it is exactly the area under the precision recall curve.\n",
    "\n",
    "### Feature Importance\n",
    "1. Multiply the feature weight and the standard deviation of the data for that feature.\n",
    "1. The columns provide little marginal signal given the other so they would both be deemed ineffective - however, removing both columns could significantly harm the performance of the model).  \n",
    "1. It may not be as bad but it still suffers from the same problem.\n",
    "1. Run PCA to automatically remove highly dependent columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
