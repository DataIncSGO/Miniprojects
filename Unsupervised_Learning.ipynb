{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "When we have a really high-dimensional dataset, we often want to lower the dimensionality of the problem.  For example, we might have a corpus of unlabeled documents and want to extract a set of topics $m$ for them.  Similarly, even a collection of 100 x 100 image would (naively) be a problem with 10,000 features.  We would like to learn lower dimensional versions of this.\n",
    "\n",
    "For unsupervised learning, we will assume a set of feature vectors $x_j$.  These correspond to the rows of the features matrix $X_{ji}$. Unsupervised learning is a type of learning that occurs when you ony have features $X = \\{X_{ji}\\}$ but not labels $y_j$.  These algorithms revolve around clustering, i.e. assigning each row $X_{j \\cdot}$ to a cluster $C_k$ such that the rows that share a cluster are more *similar* than ones from different clusters.  There are several ways to do this, some which require ground truth labeling and some which do not.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Unsupervised Learning\n",
    "\n",
    "The simplest form is **Inertia**, which is based on the 2nd moment of each cluster, (and if you are a physicist, this might be more appropriately called potential energy).  Let $\\mu_k$ be the mean of all rows in cluster $k$, i.e. for all elements $X_{j \\cdot} \\in C_k$,\n",
    "$$\\mu_k = \\frac{1}{|C_k|} \\sum_{X_{j \\cdot} \\in C_k} X_{j \\cdot}.$$\n",
    "Then the inertia is defined as the sum of the inertia (or 2nd moment) of each cluster:\n",
    "$$\\sum_k \\frac{1}{|C_k|} \\sum_{X_{j \\cdot} \\in C_k} \\| X_{j \\cdot} - \\mu_k\\|^2_2\\,.$$\n",
    "\n",
    "Another metric is called the **Silhouette Coefficient**.  If $a_j$ is the mean distance between a point $X_{j \\cdot}$ and the other points in the same cluster $C_k$ and $b_j$ is the mean is the distance between $X_{j \\cdot}$ and the other points in the next nearest cluster $C_k'$, then the coefficient is given by\n",
    "$$ \\frac{b-a}{\\max(a, b)}\\,. $$\n",
    "\n",
    "Assuming you have ground truth values $\\tilde C_k$ and predicted classification labels $C_k$, it is easy to calculate their **Mutual Information** and use those as a technique.  Mutual information has deep ties to information theory [see the Wikipedia article](http://en.wikipedia.org/wiki/Mutual_Information).  If $N$ is the total number of samples (number of rows of $X$), we can define the probabilities\n",
    "$$ P_k = \\frac{|C_k|}{N} \\qquad \\tilde P_k = \\frac{|\\tilde C_k|}{N} \\qquad P_{k,l} = \\frac{|C_k \\cap \\tilde C_l|}{N} $$\n",
    "and then the mutual information is defined as\n",
    "$$ \\sum_{k, l} P_{k,l} \\log\\left(\\frac{P_{k,l}}{ P_k \\tilde P_l }\\right)\\,. $$\n",
    "\n",
    "There are a plethora of metrics.  For more information, checkout the [Scikit Learn page on Clustering Metrics](http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation).\n",
    "\n",
    "**Questions**:\n",
    "1. How do the values of each of the measures scale with the scaling of the feature space (e.g. $X \\mapsto 2X$)?\n",
    "1. For a fixed number of points, how do each of the measures scale with the number of clusters?\n",
    "1. In the below example, write the inertia metric by hand and use an interface like `metrics.silhouette_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "# Fit KMeans onto the Iris dataset\n",
    "dataset = datasets.load_iris()\n",
    "X = dataset.data\n",
    "k_means = KMeans().fit(X)\n",
    "y = k_means.predict(X)\n",
    "\n",
    "# Compute Silhoutte score on clusters:\n",
    "print \"Silhouette Score\", metrics.silhouette_score(X, y, metric='euclidean')\n",
    "\n",
    "# Entropy\n",
    "labels_true = [1, 1, 0, 1, 2, 2, 1, 2, 0]\n",
    "labels_pred = [1, 0, 1, 1, 2, 1, 1, 2, 2]\n",
    "\n",
    "print \"Adjusted Mutual Information \", metrics.adjusted_mutual_info_score(labels_true, labels_pred)\n",
    "print \"Mutual Information \", metrics.mutual_info_score(labels_true, labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "## $K$-Means\n",
    "The specification of $K$-means is simple: assign a collection of clusters $C_k$ that minimize\n",
    "\n",
    "$$ \\mbox{argmin}_C \\sum_{k=1}^K \\sum_{X_j \\in C_k} \\| X_{j\\cdot} - \\mu_k \\|_2^2 $$\n",
    "\n",
    "where $\\mu_k$ is the center of the points in $C_k$.  The algorithm to implement this is simple:\n",
    "\n",
    "Initialize $\\mu_k$ (with possibly random values).  Then iterate between\n",
    "1. Assign $X_{j\\cdot}$ to the cluster $C_k$ that minimizes $\\|X_{j\\cdot} - \\mu_k\\|_2^2$.\n",
    "1. Recompute $\\mu_k$ by averaging over all the points $X_{j\\cdot}$ in the cluster $C_k$.\n",
    "\n",
    "Notice that both iterative steps lower the objective (the algorithm is greedy) and there are only a finite number of possible partisions of the points $X_{j\\cdot}$ so the algorithm is gauranteed to converge.  The converged solution may not be globally optimal.\n",
    "\n",
    "**Question:** What is a shortcoming of $K$-Means under scaling?\n",
    "\n",
    "## Gaussian Mixture Models\n",
    "We can generalize the notion of $K$-Means in two ways:\n",
    "1. Instead of requiring that each $X_{j\\cdot}$ strictly belong to a single $C_k$, we say it belongs to $C_k$ with a probability\n",
    "1. Insteading of just having $\\mu_k$ as a degree of freedom, we have the pair $(\\mu_k, \\Sigma_k)$\n",
    "\n",
    "Then each $X_{j\\cdot}$ is of type $k$ with probability proportional to\n",
    "$$ p_k \\exp \\left( \\frac{1}{2} (X_{j\\cdot} - \\mu_k) \\cdot \\Sigma_k^{-1}(X_{j\\cdot} - \\mu_k) \\right) $$\n",
    "where $p_k > 0$ is a proportionality constant.\n",
    "While this is not strictly a clustering algorithm, it can be turned into one but choosing the probability cluster with the highest probability.\n",
    "\n",
    "**Question**:\n",
    "1. For KMeans, what happens if the features have very different scales?\n",
    "1. Does this problem exist for Gaussian Mixture Models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster, mixture, datasets\n",
    "import pandas as pd\n",
    "\n",
    "# load Boston dataset\n",
    "diabetes = datasets.load_boston()\n",
    "\n",
    "columns = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSAT\"]\n",
    "X = pd.DataFrame(diabetes.data, columns=columns)\n",
    "y = pd.Series(diabetes.target)\n",
    "\n",
    "gmm_clf = mixture.GMM(n_components=3, random_state=42).fit(X)\n",
    "kmeans_clf = cluster.KMeans(n_clusters=3, random_state=42).fit(X)\n",
    "\n",
    "# Compute the Silhouette\n",
    "pd.DataFrame([\n",
    "    (\"GMM\",  metrics.silhouette_score(X, gmm_clf.predict(X), metric='euclidean')) ,\n",
    "    (\"KMeans\",  metrics.silhouette_score(X, kmeans_clf.labels_, metric='euclidean'))\n",
    "], columns=[\"Model\", \"Silhouette\"]).plot(x=\"Model\", y=\"Silhouette\", kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Normalize KMeans to improve performance use the scale function\n",
    "1. Test the inertia criterion you added earlier and graph it in the above plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Projections\n",
    "\n",
    "One really simple method of reducing the data is to do it at random.  That is, instead of using $X$, we use $X' = X P$ where $P$ is a $p \\times m$ matrix of randomly generated values.  This is effectively projecting the data onto the rows of $P$.  When $p \\gg m$, the rows of $P$ are highly likely to be orthogonal.  While they don't give a very faithful representation of the original data (PCA or NMF would do better), for classificaiton and regression purposes, they often give enough signal for the job.  They are much fast to compute and are good for prototyping.  They can often be a good standby as part of a learning pipeline which can be upgraded to a more principled technique as necessary.\n",
    "\n",
    "You can read more about Random Projections in the [Scikit Documentation](http://scikit-learn.org/stable/modules/random_projection.html).  We give a simple example that implements them below.\n",
    "\n",
    "**Exercise:** This projects $X$ onto a random subspace of dimension $m$ while PCA or NMF can project us onto an \"optimal\" subspace of dimension $m$.  When and why would you use random projections?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import random_projection\n",
    "\n",
    "X = np.random.rand(100, 10000)\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=30)\n",
    "X_new = transformer.fit_transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "Recall that for $X$ an $n \\times p$ matrix, we have the [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) with\n",
    "\n",
    "$$ X = U \\Sigma V^T $$\n",
    "\n",
    "where $U$ is a unitary $n \\times n$ matrix, $\\Sigma$ is a diagonal $n \\times p$ matrix, and $V$ is a unitary $p \\times p$ matrix.  We can imagine that \n",
    "\n",
    "**Exercise**: What happens if $X$ is a square and symmetric?  (The result is called spectral theorem, which roughly says $U = V^T$).  In particular, how do the SVD of $X^T X$ or $X X^T$ relate to that of $X$ (you can work out the algebra ...)\n",
    "\n",
    "## Prinicpal Components Analysis (PCA)\n",
    "\n",
    "We think of the SVD as decomposing $X$ into orthogonal (i.e. independent) components with strengths given by the diagonal of $\\Sigma$,\n",
    "\n",
    "$$ \\Sigma = \\left[ \\begin{array}{ccc} \n",
    "\\sigma_1 & \\\\\n",
    "& \\sigma_2 & \\\\\n",
    "&& \\ddots & \\\\\n",
    "&&&\n",
    "\\end{array} \\right]$$\n",
    "\n",
    "By convention, we assume $|\\sigma_1| \\ge |\\sigma_2| \\ge \\cdots$.  One interpretation of this is that $\\sigma_1$ corresponds to the largest component of variation, $\\sigma_2$ corresponds to the second largest etc ... We often think of the smaller components as being just noise and the larger components as being being signal.  Therefore, it makes sense to truncate $\\Sigma$ to its largest $m$ components and just keep those.  In Scikit, this algorithm `sklearn.decomposition.PCA` returns $U \\Sigma P_m$ where $P_m$ is the projection operator onto the first $m$ dimensions.  (In reality, it performs a stochastic SVD, which should be faster than the exact SVD).\n",
    "\n",
    "Geometrically, you can think about this as fitting an m-dimensional ellipsoid to the (originally p-dimensional) data. We expect that the majority of the variance in the data will be explained by the approximation, and any variation along the truncated axes is negligible. It's important to note that we should choose $m$ according to how much of the variance we want to preserve vs. how much compression we want.\n",
    "\n",
    "![PCA concept diagram](images/map_compression.png)\n",
    "\n",
    "We can phrase an $m$ dimensional PCA as\n",
    "$$ \\min_{U, \\Sigma_m, V} \\| X - U \\Sigma_m V^T \\|_2 $$\n",
    "where $U$ is a unitary $n \\times n$ matrix, $\\Sigma_m$ is a diagonal $n \\times p$ matrix with rank $m$, and $V$ is a unitary $p \\times p$ matrix.\n",
    "\n",
    "There's a really great brief article on PCA on [Stats StackExchange](https://stats.stackexchange.com/questions/10251/how-to-find-principal-components-without-matrix-algebra).\n",
    "\n",
    "**Exercise**:\n",
    "1. What is the answer to $\\max_{u : \\| u \\| = 1} u^T X^T X u $ and how does it relate to SVD.\n",
    "1. **Gotcha**: Why is it important to subtact the mean (mean centering) before performing PCA?\n",
    "\n",
    "## Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "Recall our minimization objective for PCA $$ \\min_{U, \\Sigma_m, V} \\| X - U \\Sigma_m V^T \\|_2 $$\n",
    "\n",
    "If we absorb the $\\Sigma_m$ into $U$ and $V^T$ as $W$ and $H$, we could rewrite this as \n",
    "$$ \\min_{W, H} \\| X - W H \\|_2 $$\n",
    "where $W$ is a $n \\times m$ matrix and $H$ is a $m \\times p$ matrix.  If $X$ has only non-negative values, we might want $W$ and $H$ to have non-negative values as well.  Hence, Non-negative Matrix Factorization is just\n",
    "$$ \\min_{W \\ge 0, H \\ge 0} \\| X - W H \\|_2 $$\n",
    "when $X \\ge 0$ (here, we use $X \\ge 0$ to mean that each element of $X$ has non-negative values).  While PCA gives you a more accurate low-dimensional representation, NMF can give a more interpretable results since the values are non-negative.\n",
    "\n",
    "This is often true of text and image data, where words and pixel value features are strictly \"positive\" and we'd like our lower-dimensional representations to also be positive. For example, it can be useful to think of text data this way:\n",
    "\n",
    "![Annotated Wikipedia NMF diagram](images/nmf_example.png)\n",
    "\n",
    "Just like in PCA, we have to choose the number of \"archetypes\" $m << n,p$ in order to compress the data, but the larger $m$, the more variance is retained.\n",
    "\n",
    "In scikit, the matrix $H$ is called the `components_` while $W$ is the value returned from `.transform`.\n",
    "\n",
    "Below, we are analyzing images of faces.  Each row is a greyscale 64 x 64 dimensional image so the feature space is 4096 dimensional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Eigenfaces using NMF and PCA\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn import decomposition\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Fetch data of faces, each face is a greyscale 64 x 64 dimensional image\n",
    "# making this a 4096 dimensional feature space\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=42)\n",
    "X = dataset.data\n",
    "\n",
    "n_col = 3\n",
    "n_row = 2\n",
    "n_components = n_col * n_row\n",
    "image_shape = (64, 64)\n",
    "\n",
    "def plot_gallery(title, images, n_col, n_row):\n",
    "    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        vmax = max(comp.max(), -comp.min())\n",
    "        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
    "                  interpolation='nearest',\n",
    "                  vmin=-vmax, vmax=vmax)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "\n",
    "estimators = [\n",
    "    decomposition.PCA(n_components=n_components, whiten=True),\n",
    "    decomposition.NMF(n_components=n_components, init='nndsvda', beta=5.0,\n",
    "                       tol=5e-3, sparseness='components'),\n",
    "]\n",
    "\n",
    "plot_gallery(\"Original Faces\", X[:6,:], n_row=n_row, n_col=n_col)\n",
    "for estimator in estimators:\n",
    "    estimator.fit(X)\n",
    "    plot_gallery(str(estimator.__class__), estimator.components_, n_row=n_row, n_col=n_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. `np.dot(pca.transform(X), pca.components_)` will project the transformed (low-dimensional) compressed features `pca.transform(X)` back to the original 4096-dimensional space.  Use to this visualize the reconstructed \"compressed\" images for different faces.  Do the same for `nmf`.  There's a lot of faces, you probably only want to visualize them for a fraction of the faces.\n",
    "1. Vary `n_components` and see how the `pca` low-dimensional reconstucted varies with different values of `n_components`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Topic modelling using NMF\n",
    "# Adapted from http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 20000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "t0 = time()\n",
    "print(\"Loading dataset and extracting TF-IDF features...\")\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                             stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(dataset.data[:n_samples])\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resricted Boltzman Machine\n",
    "\n",
    "A restricted Boltzman Machine is a type of neural-network that's used for machine learning.  It's given in the diagram of a bi-partite graph below:\n",
    "\n",
    "![RBM Graph from Scikit website](images/rbm_graph.png)\n",
    "\n",
    "The $p$ nodes labeled $v$ are the *visible* nodes and represent data (a row of $X$ in our notation).  The $m$ nodes labeled $h$ represent hidden states and the $m \\times p$ $W$ edges represent a matrix that gives the interactions between the visible and hidden nodes.  (NB: while $W$ is the standard notation for this area, it plays the analogous role to $H$ in the PCA, NMF notation above).  Why does it have a funny name?\n",
    "1.   This machine is *Restricted* because the connections between states are confined to a bipartite graph (there are no connections between $h$'s or $v$'s.  This means all its relationships can be represented by a matrix $W$.  This also implies conditional indpendence amongst the $h$'s and the $v$'s,\n",
    "\n",
    "    $$ h_i \\perp h_j | v \\qquad v_i \\perp v_j | h $$\n",
    "\n",
    "2.  This machine is *Boltzman* because for a given $v$ and $h$, we assign it an energy\n",
    "\n",
    "    $$ E(v, h) = v^T W h + b^T v + c^T h $$\n",
    "    \n",
    "    where in addition to $W$, we have the visible and hidden intercepts, $b$ and $c$.  The probability of a state is given by\n",
    "    \n",
    "    $$ P(v, h) = \\frac{e^{-E(v,h)}}{Z} $$\n",
    "    \n",
    "    where $Z$ is called a *partition function* in physics and is just the necessary coefficient to make $P$ a probability distribution.  We then train the RBM to maximize the probability\n",
    "    \n",
    "    $$ \\max_{W, b, c} P(v, h) = \\sum_h P(v, h) $$\n",
    "    \n",
    "    This is ofen done using [contrastive divergence](http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf).  It is then easy to compute\n",
    "    \n",
    "    $$ P(h | v) $$\n",
    "    \n",
    "    to obtain the hidden nodes from the visible nodes.\n",
    "\n",
    "In Scikit, `sklearn.neural_network.BernoulliRBM` maxizes log-likelihood using Stochastic Maximum Likelihood (see the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html)).  This is an iterative stochastic algorithm that takes a number of parameters,\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Parameter</th>\n",
    "<th>Meaning</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>`learning_rate`</td>\n",
    "<td>Learning Rate</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>`n_iter`</td>\n",
    "<td>Iterations to run</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>`n_components`</td>\n",
    "<td>Number of hidden states</td>\n",
    "</tr>\n",
    "</table>\n",
    "   \n",
    "   Once the model is trained, the outputs map to the mathematical notation we have expressed above members on the return object\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Math</th>\n",
    "<th>Scikit</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>$W$</td>\n",
    "<td>`.components_`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>$b$</td>\n",
    "<td>`.intercept_visible_`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>$c$</td>\n",
    "<td>`.intercept_hidden_`</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "**Exercise**: Train a RBM on the faces dataset as we did for PCA and NMF.  The hidden states are like the eigenfaces.  Draw them out and take a look at what they look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exit Tickets\n",
    "1. Explain how performing PCA or NMF affects the variance-bias tradeoff.\n",
    "1. How do you choose the k in k-means?\n",
    "1. How do you choose m in dimensionality reductions from n -> m?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers\n",
    "\n",
    "### Metrics for Unsupervised Learning\n",
    "\n",
    "1. As you scale in feature space, inertia scales as the square, Silhouette is invariant, and Mutual Information does not scale (why?).\n",
    "1. For a fixed number of points, increasing the number of clusters tends to decrease Inertia.  The effect on Silhouette is less easily determined.  Mutual information will tend to decrease as well.\n",
    "1. Straightfoward.\n",
    "\n",
    "### Clustering\n",
    "\n",
    "1. For KMeans, if the features have very different scales you should normalize the data first.\n",
    "1. This problem does not exist for Gaussian Mixture Models because of $\\Sigma$.\n",
    "\n",
    "### Random Projections\n",
    "\n",
    "1. While the subspace chosen by PCA or NMF might be \"optimal\", these algorithms are much more computationally expensive.  You would use random projects for prototyping (where speed matters) or because you have much more \"signal\" than you need to do your classification / regression job and some random subset of the signal will do.\n",
    "\n",
    "### Matrix Factorization\n",
    "1. $\\max_{u : \\| u \\| = 1} u^T X^T X u = \\sigma^2_1$ and the answer should be obvious at this point.  This should give some intuition for why $\\sigma_1$ corresponds to the largest component of variation.\n",
    "1. It important to subtact the mean (mean centering) before performing PCA.  Otherwise the first component might just be the mean of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reconstrcted faces at n_components = 10\n",
    "\n",
    "import random\n",
    "\n",
    "n_col = 3\n",
    "n_row = 2\n",
    "n_samples = n_col * n_row\n",
    "\n",
    "random.seed(42)\n",
    "X_sample = random.sample(X, n_samples)\n",
    "\n",
    "n_components = 10\n",
    "\n",
    "pca = decomposition.PCA(n_components=n_components, whiten=True).fit(X)\n",
    "nmf = decomposition.NMF(n_components=n_components, init='nndsvda', beta=5.0,\n",
    "                       tol=5e-3, sparseness='components').fit(X)\n",
    "\n",
    "def projector(M):\n",
    "    U, _, V = np.linalg.svd(M)\n",
    "    S = np.ones([U.shape[0], V.shape[1]])\n",
    "    return np.array(np.matrix(U) * np.matrix(S) * np.matrix(V))\n",
    "\n",
    "proj = projector(nmf.components_)\n",
    "\n",
    "plot_gallery(\"Before\", X_sample, n_row=n_row, n_col=n_col)\n",
    "plot_gallery(\"After NMF\", np.dot(nmf.transform(X_sample), nmf.components_), n_row=n_row, n_col=n_col)\n",
    "plot_gallery(\"After PCA\", np.dot(pca.transform(X_sample), pca.components_), n_row=n_row, n_col=n_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A reconstructed face compressed to various numbers of components\n",
    "\n",
    "import random\n",
    "\n",
    "n_col = 3\n",
    "n_row = 3\n",
    "n_show = n_col * n_row\n",
    "components_delta = 10\n",
    "\n",
    "random.seed(42)\n",
    "X_sample = random.sample(X, n_samples)\n",
    "\n",
    "n_components = 10\n",
    "\n",
    "pcas = [decomposition.PCA(n_components=n_components, whiten=True).fit(X) \n",
    "            for n_components in xrange(components_delta, n_show * components_delta, components_delta)]\n",
    "plot_data = [np.dot(pca.transform(X[23:24]), pca.components_) for pca in pcas] + [X[23]]\n",
    "plot_gallery(\"PCA at progressively higher resolution\", plot_data, n_col=n_col, n_row=n_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
