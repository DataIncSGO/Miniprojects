{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a classification algorithm that, as the name implies, the algorithm uses Bayesian statistics naively.  In particular, it supposes that all $X$'s are independent of each other and only depend on $Y$ so that\n",
    "\n",
    "$$ \\begin{array}{rl}\n",
    "p(Y | X_1, \\ldots, X_p) & \\propto p(Y) p( X_1, \\ldots, X_p | Y) \\\\\n",
    "& = p(Y) p(X_1 | Y) p(X_2 | Y, X_1) \\cdots p(X_p | Y, X_1, \\ldots, X_{p-1}) \\\\\n",
    "& = p(Y) \\prod_{i=1}^p p(X_i | Y)\n",
    "\\end{array}$$\n",
    "\n",
    "## Predictive modeling using Naive Bayes\n",
    "\n",
    "Naive Bayes is most often applied in the case where $X$ and $Y$ are discrete.  Taking the log-likelihood yields\n",
    "\n",
    "$$ \\begin{array}{rl}\n",
    "\\log p(Y = y | X_1= x_1, \\ldots, X_p = x_p) & = \\log\\left(p(Y = y) \\prod_{i=1}^p p(X_i = x_i | Y = y) \\right) \\\\\n",
    "& = \\log p(Y = y) + \\sum_{i=1}^p \\log p(X_i = x_i | Y = y)\n",
    "\\end{array}$$\n",
    "\n",
    "Algorithmically, it is easy to estimate because we can use simple counting to estimate $p(Y = y)$ and $p(X_i = x_i | Y = y)$ as $X_i$ and $Y$ are both categorical variables.  For example,\n",
    "\n",
    "$$p(Y = y) = \\frac{\\mbox{number of samples where label is }y}{\\mbox{number of samples}}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$p(X_i = x_i | Y = y) = \\frac{\\mbox{number of samples where feature }X_i\\mbox{ is }x_i \\mbox{ and }Y\\mbox{ is } y}{\\mbox{number of samples where label is }y}$$\n",
    "\n",
    "**Question**:\n",
    "\n",
    "1. Naive Bayes intentionally makes an unreaslitic assumption about $X_1, \\ldots, X_p$ being independent to generate a linear model.  What is a similar model which doesn't rely on this assumption?  Why do we not use it instead of Naive Bayes?\n",
    "1. What is the value of $p(X_i | Y)$ if there are no examples in this class?  How would you fix this?\n",
    "1. Naive bayes intentionally makes an unreaslitic assumption about $X_1, \\ldots, X_p$ being independent.  We could easily capture the correlation structure just by computing the full \"tensor\" of possibilities.  For example, for $p=2$, we would fill in the counts for each class of $Y$ in the matrix\n",
    "\n",
    "<table>\n",
    "  <col width=\"80\">\n",
    "  <col width=\"80\">\n",
    "  <col width=\"80\">\n",
    "<tr>\n",
    "<td> </td>\n",
    "<td>$X_1=A$</td>\n",
    "<td>$X_1=B$</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>$X_2=C$</td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>$X_2=D$</td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Why would you not do this?\n",
    "\n",
    "In Sklearn, the Naive Bayes \n",
    "can be found [here](http://scikit-learn.org/stable/modules/naive_bayes.html).  The algorithm are called:\n",
    "1. `sklearn.naive_bayes.GaussianNB` (for continuous features, not described here)\n",
    "1. `sklearn.naive_bayes.MultinomialNB` (for discrete features).\n",
    "1. `sklearn.naive_bayes.BinomialNB` (for discrete features that are always binomial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exit Tickets\n",
    "1. Describe how you would implement Naive Bayes using MapReduce.\n",
    "1. What hyperparameters might you need to tune using cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "1. Linear Regression requires loading all the data into memory.  Naive Bayes can work on streaming data and hence can be used in parallelized algorithms like Mapreduce.\n",
    "1. Without data, the conditional probability is not defined.  The way you get around this is called Bayesian smoothing.  For example,\n",
    "\n",
    "   $$p(Y = y) = \\frac{\\mbox{number of samples where label is }y + p_0 n_0}{\\mbox{number of samples} + n_0}$$\n",
    "\n",
    "   where we assume some \"prior\" of rate $p_0$ with $n_0$ observed examples.\n",
    "\n",
    "1. If each of $X_1,\\ldots,X_n$ can take on $K$ possible values, then Naive Bayes has $Kp$ \"bins\" to fill.  The full \"tensor\" would have $K^p$ possibilities and most of those cells would be empty since it is likely that $n < K^p$.  This exponential growth in the number of \"bins\" is called the curse of dimensionality.  On the other hand, if $n \\gg K^p$ you might want to do the full tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
