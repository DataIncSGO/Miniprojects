{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the data really says\n",
    "---------------------\n",
    "\n",
    "### Summary data lies\n",
    "\n",
    "Who is a better batter?  (and how to compute the standard error?)\n",
    "\n",
    "|               |       Combined |\n",
    "|---------------|----------------|\n",
    "| Derek Jeter   | **.310** (630) |\n",
    "| David Justice |   .270 (551)   |\n",
    "\n",
    "Now who is a better batter?\n",
    "\n",
    "|               |           1995 |           1996 |       Combined |\n",
    "|---------------|----------------|----------------|----------------|\n",
    "| Derek Jeter   |   .250 (48)    |   .314 (582)   | **.310** (630) |\n",
    "| David Justice | **.253** (441) | **.321** (140) |   .270 (551)   |\n",
    "\n",
    "**Questions:**\n",
    "1. There are two types of uncertainty here.  One is statistical uncertainty.  This decreases with $1/\\sqrt{n}$.  The other is epistemic uncertainty (even with the 2nd chart, can we be sure that Justice is a better player?).  Having more data does not necessarily help.\n",
    "\n",
    "1. How does this relate to randomized control trials?  What type of error would a randomized control trial solve?\n",
    "\n",
    "\n",
    "### Correlation and causation\n",
    "In 1973, UC Berkeley's admissions stats showed:\n",
    "\n",
    "|            | Applications |    Admitted |\n",
    "|------------|--------------|-------------|\n",
    "|        Men |         8442 |         44% |\n",
    "|      Women |         4321 |         35% |\n",
    "\n",
    "Was the university biased in its admissions?  See [Simpson's Paradox](http://en.wikipedia.org/wiki/Simpson%27s_paradox)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fidelity\n",
    "----------------\n",
    "\n",
    "Data generators and data producers are different people.  Sometimes there's no feedback loop between them.\n",
    "\n",
    "### Did one consistent source generate the data or did multiple?\n",
    "1. Do keywords always mean the same thing?  Topic tags on articles often have a large number of contributors with unsynchronized taxonomies.  Some websites, e.g. Wikipedia, have reasonably right editorial guidelines, but in general this is not strict.\n",
    "1. Sometimes, there are implicit tags that don't appear.  For example, most restaurants in Japan would probably be considered \"Japanese,\" but most restaurants aren't labeled as \"Japanese\" because they are already implicitly assumed to be by their patrons.\n",
    "\n",
    "### Was the data processed by a third party that inadvertently obscured certain fields?  How can you tell?\n",
    "1. Sometimes null data is encoded as a special number (e.g. 9999, -1, 0) but this can lead to ambiguity because those could always be valid data values. **Question:** How can you distinguish between them?\n",
    "1. Sometimes data processing goes amuck and data is just silently dropped.  **Question:** How can tell if some data was silently dropped in a data file?\n",
    "\n",
    "### Systematic (pathological?) ways in which people interact with the system that cause biases in the data\n",
    "1. **Rounding:** Where true value of the data is obscured by rounding.  If you poll people on how much money they make annually, you'll likely find values divisible by \\$1,000 or \\$5,000 show up disproportionately often.  **Question:** Is this reflective of reality?  How would you deal with it?\n",
    "1. **Human Habits:** Sometimes, what appear to be rounded values might not have been rounded.  If you look at the histogram of the number of shares executed per trade, you'll see huge peaks at 100 or 500 shares.  People like round numbers.  Similarly, intervals between trades are often 1 sec or 30 secs.\n",
    "1. **Spurious visits:** looking at web logs, you'll often see the same user visit a webpage multiple times within a few seconds.  You often want to consider that as one visit because they are probably refreshing their browser due to a slow connection.  For similar reasons, you'll get multiple check-ins at the same venue within a few minutes.\n",
    "\n",
    "### External factors that influence your data\n",
    "\n",
    "Sometimes, completely external factors can influence an outcome:\n",
    "\n",
    "1. Presidential elections see a spike in babies born with the president's first name.\n",
    "1. If you are analyzing the effect of an online banner ad campaign, was there a TV campaign while you were showing your banner ad?\n",
    "1. The stock market is a classic example of where external events can have a huge influence on data.\n",
    "1. In housing data, credit standards were loosened leading up to '08 which led to the spike in defaults when the economy collapsed.\n",
    "\n",
    "### Unique codes are not always unique\n",
    "1. Congressional districts and zip codes get shuffled around.  Be careful when inferring facts like \"the 3rd Congressional District of some state will vote red because historically it's very Republican.\"\n",
    "1. Similarly, stock IDs (which are supposed to be a stable way to identify companies across time periods) change due to mergers and acquisitions.\n",
    "\n",
    "### Joining data sets from different sources\n",
    "1. Sometimes you can have unique keys provided by third parties.  For example, there are a few schemas for security IDs and venue IDs provided by different third parties.  Social Security Numbers and Employer ID Numbers are meant to be uinque keys provided by the Social Security Administration and the Internal Revenue Service.  **Question:** How would you join records about companies or individuals without such a unique key?\n",
    "1. If one of the databases is at a coarser level of granularity, e.g. zip code or household level, you might be able to join the data based on that level by aggregating another dataset to that level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data relevance\n",
    "-----------------\n",
    "\n",
    "### Temporal or cohort effects (especially in economic data):\n",
    "1. If you ran a trading strategy from '92 - '00 and then '02 - '07, you probably would have made a lot of money.  That doesn't mean you would have made money '00 - '02 or '08 - '10.  There's a huge debate in economics about the validity of the \"Ergodicity Assumption\".\n",
    "1. Similarly, with housing data the underlying credit underwriting standards changed in ways that mean there are really strong cohort effects.\n",
    "1. In website user data, we often get different cohort effects: early users might be those who were drawn to early features and be more reluctant to adopt new features.  The website or app changes with time so different cohorts experienced different lifecycles of the product.\n",
    "**Question:** How do you control for this?\n",
    "\n",
    "### Is your data available at decision time?\n",
    "1. Example: For a search engine, the position at which a result is displayed affects the probability of getting a click.  This is a natural feature to incorporate in a model.  However, that information is not available at the time of decision making.  **Question:** how do you control for this when estimating a model?\n",
    "\n",
    "### What is the time duration and frequency of your data?\n",
    "1. **Mismatched data:** It is easy to get ~ week of Twitter data by sampling the fire hose.  You have millisecond frequency data at a few weeks' duration.  It is easy to collect stock market data from free sources at a daily frequency but with decades of duration.  Unfortunately, when you try to join these two datasets at the finest common level at the longest common interval, the intersection would be a few weeks of daily data, which is not that impressive.\n",
    "1. **Decision making time scale:**  If you are making quarterly marketing decisions for a CPG company, you don't need up-to-the-minute Twitter data.  Similarly, if you need to make secondly decisions about what product to display on an E-Commerce site, consumer spending data released quarterly by the government might not be that useful.\n",
    "    \n",
    "### How do you calculate the value of new data?\n",
    "1. If you generate a new signal, how would you quantify its value?  It's easy to say \"it decreased MSE by _\" but we often care about an impact in terms of business metrics.  \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Data Science Metric</th>\n",
    "<th>Business Metric</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>MSE for predicting tomorrow's stock price</td>\n",
    "<td>How much more we might make</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Entropy of email spam detector on labeled testing set</td>\n",
    "<td>Number of new spam messages we caught or didn't (change in true positives) and number of falsely classified spam messages (false positive) per thousand</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Identifying likely customers</td>\n",
    "<td>How many additional customers would your algorithm bring in?</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "It is harder to explain the value in business terms.  You can determine its value?\n",
    "1. How do you translate Metric Lift to Revenue Lift?\n",
    "\n",
    "**Exercise:**\n",
    "1. What if you had new data for identifying customers?\n",
    "1. What is the new value of data in the stock market?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling tradeoffs\n",
    "-----------------------\n",
    "\n",
    "### Are we looking at the right scientific metric?\n",
    "\n",
    "1. Credit card mailers looking at response rates to mailers.  Unfortunately, what we really care about is profitability, and bad customers and good customers look alike.\n",
    "1. At Foursquare, we were very concerned about new venue conversions versus someone going to a search result that they had been to before.\n",
    "\n",
    "### Fidelity of metric and time-delay of metric\n",
    "\n",
    "### Tradeoff between Accuracy and Interpretability\n",
    "\n",
    "1. **Question:** When would you emphasize each? Early in business development cycle versus later in the business development cycle?\n",
    "1. **Question:** How much do you trust your metrics and how does this impact the tradeoff?  In ads, we were comfortable using a Random Forest.  In recommendation systems, we were not because we wanted a lot of human input.\n",
    "\n",
    "### How much data do you have?\n",
    "\n",
    "1. How does data accuracy impact how accurate your model can be?\n",
    "\n",
    "### Are there going to be large structural shifts in the world?\n",
    "\n",
    "If you know that a website is undergoing a huge renovation or the housing market's dynamics will change in light of 2008, there's no reason to build a fancy model based on historical data because it will become quickly irrelevant.  Using simple mean estimates from more representative samples will be more explainable and easier to build.\n",
    "\n",
    "### Tradeoff between Precision and Recall\n",
    "\n",
    "1. What's the cost of a false negative versus a false positive?\n",
    "\n",
    "### How much time do you have to make a decision?\n",
    "1. HFT versus Stat Arb.\n",
    "1. If you want to do a fancy calculation but don't have enough time to do it, what do you do?\n",
    "\n",
    "### Are we looking at the right segment of the data?\n",
    "1. When trying to look at expected credit card losses, do you want to examine delinquent or non-delinquent users?\n",
    "1. Similarly, when trying to estimate the expected cost of hospitalization stays, do you want to look at the segment that has previously been hospitalized or are otherwise at risk?\n",
    "\n",
    "### Is there a part of the population that's irrelevant to the problem you're trying to solve?  Let's not even analyze it very much.\n",
    "\n",
    "- Collections?  Ignore people who will always pay or never pay?\n",
    "- Advertising?  Ignore people who will never use the product (guys and lipstick) and people who already do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protecting privacy\n",
    "---------------------------------\n",
    "\n",
    "1. Making sure your aggregation pools are big enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this a zero sum game?\n",
    "---------------------------\n",
    "\n",
    "1. Batting averages versus record time to run a mile.\n",
    "1. How would you classify ad targeting?  Trading?  Credit Modeling (consumer finance)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fidelity?\n",
    "\n",
    "### Was the data processed by a third party that inadvertently obscured certain fields?  How can you tell?\n",
    "1. How can you distinguish between them?  Histograms and thinking really hard about why.\n",
    "1. How can you tell if some data was silently dropped in a data file?  Splice the data and ensure that they look similar.  E.g. with stock data: are there gaps in users for certain stocks?  For purchase logs, are there certain customer segments that are missing on certain days?\n",
    "\n",
    "1. How would you join records about companies or individuals without such a unique key?  For individuals, you might consider name, sex, phone number, email, and occupation as identifying characteristics that, when combined, might give you a key.  For companies, you might consider name and address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets\n",
    "1. Name three data relevance considerations when dealing with time series.\n",
    "1. You're given a pandas dataframe that doesn't seem to have any None entries. How can you make sure that there is no mis-entered data?\n",
    "1. Your team is excited about a new data gathering initiative which is going to require a large engineering investment. How do determine the expected impact of more data on your model? How would you recommend the team move forward in both contingencies?\n",
    "1. How could you systematically apply the ideas in this notebook to real-world projects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
