{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spark applications with PySpark\n",
    "\n",
    "The PySpark API is similar to Scala, but not exactly, and there may be missing features. Be careful when looking through the [documentation](https://spark.apache.org/docs/latest/programming-guide.html).\n",
    "\n",
    "PySpark comes installed with all versions of Spark, and you should be able to `import pyspark` without any trouble.\n",
    "For machine learning applications, you'll probably need to work within a Spark SQL context as well as the usual Spark Context - this is to enable DataFrame functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"main\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "lines = sc.textFile(\"small_data/gutenberg/\")\n",
    "totalLines = lines.count()\n",
    "print \"total lines: %d\" % totalLines\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run locally:\n",
    "\n",
    "`$SPARK_HOME/bin/spark-submit --py-files src/aux_classes.py src/main.py arg1 arg2`\n",
    "\n",
    "## Dependencies and class definitions\n",
    "\n",
    "Use the `--py-files` flag with `spark-submit` to specify additional Python modules which should be made available to each worker. These may include class definitions or third-party dependencies. Usually, if you're using classes, you will not be able to define them in the main file.\n",
    "\n",
    "## PySpark on EMR\n",
    "\n",
    "The setup is similar to Scala, although you will need to manage your dependencies through the --py-files flag. Since you can pass python code to `spark-submit`, you can simply use the script-runner JAR to start the cluster. Documentation is available [here](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-spark-submit-step.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spark applications with Scala\n",
    "\n",
    "While the interactive console is fun, it is (likely) not how you will be submitting a job.  Instead, you will want to follow these steps.  A sample simple application is provided in [projects/simple-spark-project](projects/simple-spark-project).\n",
    "\n",
    "1. **Build your Spark application**: Scala is a compiled language so you will need to build a jar that can be run on the Java Virtual Machine (JVM).  JAR (Java Archive) is a package file format typically used to aggregate many Java class files and associated metadata and resources (text, images, etc.) into one file to distribute application software or libraries on the Java platform.  Go to the project directory `projects/simple-spark-project` and run\n",
    "```bash\n",
    "$ sbt package\n",
    "```\n",
    "\n",
    "2. Submit the job locally running on 4 cores:\n",
    "```bash\n",
    "$ $SPARK_HOME/bin/spark-submit \\\n",
    "  --class \"com.thedataincubator.simplespark.SimpleApp\" \\\n",
    "  --master local[4] \\\n",
    "  target/scala-2.10/simple-project_2.10-1.0.jar\n",
    "```\n",
    "\n",
    "You can use local[*] to run with as many worker threads as logical cores on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging with `sbt`\n",
    "\n",
    "### What is `sbt`?\n",
    "`sbt` is a modern build tool written in and for Scala, though it is also a general purpose build tool.  `sbt` is actually a Scala [Domain Specific Language (DSL)](https://en.wikipedia.org/wiki/Domain-specific_language), meaning it's actually Scala (with enough new constructs to look like it's not Scala).  To invoke SBT, run\n",
    "``` bash\n",
    "$ sbt\n",
    "```\n",
    "which brings you into an \"sbt session.\"  The commands given below are to be typed within an SBT session.\n",
    "\n",
    "### Why `sbt`?\n",
    "- Sane(ish) dependency management\n",
    "- Incremental recompilation and keeping the compiler alive in between compilations (see [this article](http://www.scala-sbt.org/0.13.2/docs/Detailed-Topics/Understanding-incremental-recompilation.html))\n",
    "- Automatic recompilation triggered by file-change.  Within an sbt session, enter:\n",
    "    ```\n",
    "    ~compile\n",
    "    ```\n",
    "- Run the program within sbt:\n",
    "    ```\n",
    "    run\n",
    "    ```\n",
    "- Test the program within sbt:\n",
    "    ```\n",
    "    test\n",
    "    ```\n",
    "- Full Scala language support for creating tasks (it's a DSL)\n",
    "- Launch REPL in project context\n",
    "    ```\n",
    "    console # gives you a Scala repl within your jar\n",
    "    ```\n",
    "    and you can type commands into the REPL to play around\n",
    "    ```scala\n",
    "    import com.thedataincubator.simplespark.SimpleApp\n",
    "\n",
    "    val x = 1\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Layout (Directory Structure)\n",
    "\n",
    "A sample simple application is provided in [projects/simple-spark-project](projects/simple-spark-project).\n",
    "\n",
    "### Source files:\n",
    "1. `src/main` – your app code goes here, in a subdirectory indicating the code’s language, e.g.\n",
    "    1. `src/main/scala`\n",
    "    1. `src/main/java`\n",
    "1. `src/main/resources` – static files you want added to your jar (e.g. logging config)\n",
    "1. `src/test` – like src/main, but for tests  \n",
    "1. `src/main/scala/com/thedataincubator/simplespark/SimpleApp.scala` - an actual code file.  This is in two components:\n",
    "    1. `src/main/scala` - overhead (explained above)\n",
    "    1. `com/thedataincubator/simplespark/` - related to the package hierarchy (b/c it's written by people at the domain `thedataincubator.com`).  There are two files in our sample app:\n",
    "        1. `src/main/scala/com/thedataincubator/simplespark/SimpleApp.scala` - main app\n",
    "        1. `src/main/scala/com/thedataincubator/simplespark/Foo.scala` - helper class and methods\n",
    "    \n",
    "    This affects your code in two places:\n",
    "        1. All your `*.scala` files in this directory need to declare their packages consistently with their directory\n",
    "        ```scala\n",
    "        package com.thedataincubator.simplespark\n",
    "        ```\n",
    "        You can then easily access other files in this folder (package).  For example, there is also a `Foo.scala` in this directory (with the same package definition) and we can access it directly in `SimpleApp.scala` \n",
    "        1. When you invoke the jar, you need to specify the class (package) name (see the `spark-submit` command above)\n",
    "\n",
    "### Build Files:\n",
    "1. `build.sbt` - This is like a make file.  It tells `sbt` how to build your project.  You can specify the version of your application, the Scala version you want, and the version of the dependencies you require (e.g. Spark) in the `build.sbt`:\n",
    "    ```scala\n",
    "    name := \"Simple Project\"\n",
    "    version := \"1.0\"\n",
    "    scalaVersion := \"2.10.4\"\n",
    "    libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.2.0\"\n",
    "    ```\n",
    "1. `project/` – Because the `sbt` compiler is actually Scala code, the compiler has to be built.  It is built with `sbt`.  The instructions for how to build this meta-build are placed in this directory.  This allows you to tweak the build's build.\n",
    "1. `project/build.sbt` - The instructions for the meta-build (like `build.sbt` but for the compiler, not for your main project).  You can also tweak the build's build's build by having `project/project` and continue iterating forever (see [Organizing Build's](http://www.scala-sbt.org/0.13/tutorial/Organizing-Build.html)).\n",
    "\n",
    "### Output files:\n",
    "1. `target/` – The destination for generated files (e.g. class files, jars).\n",
    "\n",
    "### Further information:\n",
    "For more, check out the [documentation](http://www.scala-sbt.org/0.13/tutorial/Directories.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark on EMR\n",
    "This section will describe how to run a Spark jar as part of the SparkOverflow project. It assumes you are using stevenskelton's class structure and the spark-overflow_2.10-1.0.jar.\n",
    "\n",
    "You'll first need to create a folder on s3 called target, which contains your packaged jar and the classes folder (the contents of the target/scala-2.10/ folder after running sbt package). Run the create_spark_cluster script (~/datacourse/scripts/) with target's parent directory as the argument, e.g. \n",
    "\n",
    "``` bash\n",
    "$ python create_spark_cluster.py s3://thedataincubator-fellow/tempFellow/spark/\n",
    "```\n",
    "\n",
    "The full path to the jar would be `s3://thedataincubator-fellow/tempFellow/spark/target/spark-overflow_2.10-1.0.jar`\n",
    "\n",
    "The script will create folders in your base directory called logs/ and output/ - make sure these are empty or nonexistent before running.\n",
    "\n",
    "*Please don't alter the script!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final gotcha\n",
    "\n",
    "Scala (like Java) requires everything to be inside a class or object.  The repl will accept a global assignment like\n",
    "```scala\n",
    "val x = 1\n",
    "```\n",
    "but this will not work in application code that you compile.  You would need to put this into an object or class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
