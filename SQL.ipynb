{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL\n",
    "Structured Query Language (pronounced like \"sequel\")\n",
    "\n",
    "**Key nouns:**\n",
    "    - \"Relational\" database\n",
    "    - Table\n",
    "    - Row\n",
    "    - Column\n",
    "    - (Primary key)\n",
    "\n",
    "**Key verbs:**\n",
    "    - `CREATE TABLE` / `DROP TABLE` to modify the structure of the database\n",
    "    - `SELECT` to read out some rows\n",
    "    - `INSERT` / `UPDATE` / `DELETE` to modify (or delete) some rows of tables\n",
    "\n",
    "**Key modifiers:**\n",
    "    - `WHERE`: impose condition\n",
    "    - `ORDER BY`: sort\n",
    "    - `GROUP BY`: aggregate (e.g. pivot tables in Excel)\n",
    "    - `JOIN`: combining tables in one query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The relational data model\n",
    "\n",
    "Here's a diagram of a simple database for employee work-planning in the \"relational model\":\n",
    "\n",
    "<div>\n",
    " <div>\n",
    " <div>\n",
    " <table>\n",
    " \n",
    " <tr>\n",
    "  <th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;employee_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </th> \n",
    " <th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </th>\n",
    " <th> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </th>\n",
    " <th> &nbsp;&nbsp;Phone&nbsp;&nbsp;&nbsp;  </th>\n",
    " </tr>\n",
    " \n",
    " <tr>\n",
    " <td>1</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jenny Smith&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </td>\n",
    " <td> &nbsp;&nbsp;867-5309&nbsp;&nbsp;&nbsp;  </td>\n",
    " </tr>\n",
    "\n",
    " <tr>\n",
    " <td>2</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;John Doe&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </td>\n",
    " <td> &nbsp;&nbsp;555-1234&nbsp;&nbsp;&nbsp;  </td>\n",
    " </tr>\n",
    "\n",
    " <tr>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " </tr>\n",
    "\n",
    " <tr>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " </tr>\n",
    "</table>\n",
    "</div>\n",
    "<div style=\"text-align: center\"> <span> Table: <tt>Employees</tt> </span>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<br />\n",
    "\n",
    " <div>\n",
    " <div>\n",
    " <table>\n",
    " \n",
    " <tr>\n",
    " <th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;task_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </th> \n",
    " <th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;deadline&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </th>\n",
    " <th> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </th>\n",
    " <th> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </th>\n",
    " </tr>\n",
    " \n",
    " <tr>\n",
    " <td>1</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014-07-18&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </td>\n",
    " <td> &nbsp;&nbsp;Save the world.&nbsp;&nbsp;&nbsp;  </td>\n",
    " </tr>\n",
    "\n",
    " <tr>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " </tr>\n",
    "</table>\n",
    "</div>\n",
    "<div style=\"text-align: center\"> <span> Table: <tt>Tasks</tt> </span>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<br />\n",
    "\n",
    "<div>\n",
    " <div>\n",
    " <table>\n",
    " \n",
    " <tr>\n",
    " <th> &nbsp;&nbsp;assignment_id&nbsp;&nbsp;&nbsp;&nbsp; </th> \n",
    " <th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;task_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </th> \n",
    " <th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;employee_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </th>\n",
    " <th> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;start_date&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </th>\n",
    " <th> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;end_date&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </th>\n",
    " <th> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </th>\n",
    " </tr>\n",
    " \n",
    " \n",
    " <tr>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    " </tr>\n",
    " \n",
    " <tr>\n",
    " <td>1234</td>\n",
    " <td>1</td>\n",
    " <td>2</td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014-05-18&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014-07-18&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </td>\n",
    " <td> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  </td>\n",
    " </tr>\n",
    "\n",
    "</table>\n",
    "</div>\n",
    "<div style=\"text-align: center\"> <span> Table: <tt>Assignments</tt> </span>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "The database consists of:\n",
    " - **Tables**: Employees, Tasks, and Assignments.  \n",
    "      > Each table represents a different kind of \"relation\" between bits of data.\n",
    "    \n",
    " - Each of the tables has many **rows** (sometimes called \"relations\").  For instance, each row of the `Employees` table represents information about a single employee:  \n",
    " \n",
    "      > Elements of a single column are _homogeneous_ (of the same kind, or data type) but apply to _different logical entities_.\n",
    "      \n",
    " - The information about a single employee is spread amongst the **columns** of the table:\n",
    " \n",
    "      > Elements of a single row are _heterogeneous_ but apply to _a single logical entity_.\n",
    "      \n",
    " - It is good practice to be able to specify each row of each table by a **primary key**: that is by some column, or small combination of columns, that uniquely specifies the row.  The columns `employee_id`, `task_id`, and `assignment_id` are all of this type.  This way, if you want to refer to this row in another table you can do it succinctly by this primary key.\n",
    "      \n",
    "Here are some examples of types of \"relations\" (rows) that occur in practice:\n",
    "   1. Storing one-to-one bits of data for some sort of object: i.e. for each employee we store one name and one phone number; for each project we store one deadline and one description\n",
    "   2. Storing one-to-many or many-to-many bits of data: i.e. if we needed to be able to store _many_ phone numbers for each employee, we'd have a new table `Employee_Phone_Numbers` with columns `employee_id` and `phone_number` (the two together can be a _composite primary key_).\n",
    "    \n",
    "   Alternate, but **worse**, ways to do it would be: \n",
    "       - Adding columns to the `Employees` table for `phone_number_1` to `phone_number_10`.  This is bad because it makes it hard to query against the phone numbers and is inflexible if you want to change the number of phone numbers.  In the _object oriented_ or _key-value_ you could simply make phone_numbers a list or array, and all would be well -- this is not available in the relational model. \n",
    "       - Having the `Employee_Phone_Numbers` table duplicate the employee name and other information.  This is bad because duplication of information across tables wastes space and can lead to update errors.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by doing all our examples in `sqlite`.   Then at the end we'll talk a little bit about setting up and using `postgres`.  They both speak (mostly) SQL, so at the end we'll talk about why you might use one of them (vs the other, and vs other options).\n",
    "\n",
    "Both have command-line clients (`sqlite3` and `psql`) but there is also a convenient IPython %sql \"magic\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas and creating databases\n",
    "A __schema__ is a description of the structure of a database (tables, rows, etc.).  We proceed by example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql sqlite://\n",
    "DROP TABLE IF EXISTS Employees;\n",
    "CREATE TABLE Employees (\n",
    "    employee_id INTEGER PRIMARY KEY,\n",
    "    name TEXT NOT NULL,\n",
    "    hire_date DATE NOT NULL,\n",
    "    title TEXT NOT NULL,\n",
    "    phone_number TEXT NOT NULL\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS Tasks;\n",
    "CREATE TABLE Tasks (\n",
    "    task_id INTEGER PRIMARY KEY,\n",
    "    deadline DATE NOT NULL,\n",
    "    status INT NOT NULL DEFAULT 0,\n",
    "    description TEXT\n",
    ");\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS Assignments;\n",
    "CREATE TABLE Assignments (\n",
    "    assignment_id INTEGER PRIMARY KEY,\n",
    "    task_id INTEGER NOT NULL,\n",
    "    employee_id INTEGER NOT NULL,\n",
    "    start_date DATE NOT NULL,\n",
    "    end_date DATE NOT NULL,\n",
    "    charge_code TEXT NOT NULL,  -- This should probably be its own table!\n",
    "    fraction_assignment REAL NOT NULL\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS Pension_Choices;\n",
    "CREATE TABLE Pension_Choices (\n",
    "    pension_id INTEGER PRIMARY KEY,\n",
    "    employee_id INTEGER NOT NULL, \n",
    "    plan_name TEXT NOT NULL,\n",
    "    funding_amount INTEGER NOT NULL);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting data\n",
    "The basic way to create a new row is\n",
    "\n",
    "    INSERT INTO table_name [(column_name_1, column_name_2, ...)] VALUES (value_1, value_2, ...);\n",
    "\n",
    "If you don't give the optional list of column names, then it is assumed that the values are in the order that the fields appeared in the `CREATE TABLE` statement.  Giving the list of column names is a good idea: It doesn't break when you change your schema, it lets you take advantage of default values and auto-incrementing primary keys, etc.\n",
    "\n",
    "**Gotchas:**\n",
    " - For strings you 'must' use single quotes.  (SQLite and MySQL don't care.  Postgres does.)\n",
    " - The primary key is required to uniquely specify a row.  If you try to insert something that violates this, you will get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO Employees VALUES (1, 'Jenny Smith', '2010-12-1', 'Chief Music Office', '867-5309');\n",
    "INSERT INTO Employees (title, name, phone_number, hire_date) VALUES ('CEO', 'John Doe', '555-1234', '2011-12-1');\n",
    "INSERT INTO Employees (title, name, phone_number, hire_date) VALUES ('Worker', 'Worker Bee 1', '555-1201', '2012-12-01');\n",
    "INSERT INTO Employees (title, name, phone_number, hire_date) VALUES ('Worker', 'Worker Bee 2', '555-1202', '2012-12-03');\n",
    "INSERT INTO Employees (title, name, phone_number, hire_date) VALUES ('Worker', 'Worker Bee 3', '555-1203', '2012-12-05');\n",
    "INSERT INTO Employees (title, name, phone_number, hire_date) VALUES ('Worker', 'Worker Bee 4', '555-1204', '2012-12-07');\n",
    "\n",
    "INSERT INTO Tasks (deadline, description, status) VALUES ('2012-12-11', 'Hire worker bees.', 1);\n",
    "INSERT INTO Tasks (deadline, description) VALUES ('2014-07-18', 'Save the world.');\n",
    "INSERT INTO Tasks (deadline, description, status) VALUES ('2014-12-11', 'Cost cutting on worker bees.', 0);\n",
    "\n",
    "\n",
    "INSERT INTO Assignments (task_id, employee_id, start_date, end_date, fraction_assignment, charge_code) VALUES (1, 2, '2012-01-01', '2012-12-31', 0.5, 'INTERNAL');\n",
    "INSERT INTO Assignments (task_id, employee_id, start_date, end_date, fraction_assignment, charge_code) VALUES (2, 3, '2013-01-01', '2014-07-18', 1.0, 'CJX5');\n",
    "INSERT INTO Assignments (task_id, employee_id, start_date, end_date, fraction_assignment, charge_code) VALUES (2, 4, '2013-01-01', '2014-07-18', 1.0, 'CJX5');\n",
    "INSERT INTO Assignments (task_id, employee_id, start_date, end_date, fraction_assignment, charge_code) VALUES (2, 5, '2013-01-01', '2014-07-18', 1.0, 'CJX5');\n",
    "INSERT INTO Assignments (task_id, employee_id, start_date, end_date, fraction_assignment, charge_code) VALUES (2, 6, '2013-01-01', '2014-07-18', 1.0, 'CJX5');\n",
    "INSERT INTO Assignments (task_id, employee_id, start_date, end_date, fraction_assignment, charge_code) VALUES (2, 2, '2013-01-01', '2014-07-18', 0.75, 'CJX5');\n",
    "INSERT INTO Assignments (task_id, employee_id, start_date, end_date, fraction_assignment, charge_code) VALUES (3, 2, '2013-01-01', '2014-07-18', 0.25, 'INTERNAL');\n",
    "\n",
    "INSERT INTO Pension_Choices (employee_id, plan_name, funding_amount) VALUES (3, 'Retirement Plan A', '3000');\n",
    "INSERT INTO Pension_Choices (employee_id, plan_name, funding_amount) VALUES (4, 'Retirement Plan A', '5000');\n",
    "INSERT INTO Pension_Choices (employee_id, plan_name, funding_amount) VALUES (5, 'Retirement Plan B', '5000');\n",
    "INSERT INTO Pension_Choices (employee_id, plan_name, funding_amount) VALUES (6, 'Retirement Plan B', '1000');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the primary key is required to be _unique_.  That means that if we were to try the following line\n",
    "```SQL\n",
    "INSERT INTO Employees VALUES (1, 'Jenny Smith', '2010-12-1', 'Chief Music Office', '867-5309');\n",
    "```\n",
    "_again_, it would fail.  (Even if the fields after the 1 were changed to be different.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data\n",
    "The basic way to read rows from a single table is\n",
    "\n",
    "```SQL\n",
    "SELECT expression1, expression2, ... FROM table_name [WHERE condition] [GROUP BY columns] [ORDER BY columns] [LIMIT number];\n",
    "```\n",
    "\n",
    "Brackets denote parts which are optional.  Let us explain the parts:\n",
    "   1. __\"Expressions\": __\n",
    "   \n",
    "   The basic example is just a column name.  Another example is the placeholder `*`: it indicates _all_ columns.  You can also perform arithmetic (e.g. `column_1 + column_2`) and similar minor operations.\n",
    "   2. __`WHERE` (\"condition\"):__\n",
    "   \n",
    "   An example would be `column_name = value`, and for partial string matches there is a special operator `LIKE`.  You can combine these with the usual binary operators (AND, OR) and parentheses for grouping.\n",
    "   3. __`GROUP BY` (aggregation):__\n",
    "   \n",
    "   Add a `GROUP BY column_name1, ...` term to aggregate the results along the listed columns.   This means that for _other_ columns you mention in the expressions you must explicitly specify an _aggregate method_ by writing e.g. `SUM(column_1)`.\n",
    "   \n",
    "   You can use these aggregate methods even without a GROUP BY, to get information for the whole table: e.g.\n",
    "  ```SQL \n",
    "SELECT COUNT(1) FROM table_name;\n",
    "SELECT SUM(colum) FROM TABLE_name;\n",
    "   ```\n",
    "   return the number of rows, and the sum of the named column, respectively.\n",
    "   \n",
    "   4. __`ORDER BY` (sorting):__\n",
    "   \n",
    "   You can add a `SORT BY column_name1, column_name2, ...` to sort the results by the given column (applied in left-to-right order).  By default the sorting is ascending.  If you want it to be descending write e.g. `column_name1 DESC` (you can also write `ASC` for ascending to disambiguate).\n",
    "   \n",
    "   5. __`LIMIT`:__\n",
    "   \n",
    "   If you want to just get at most the first N rows that would be returned, add on `LIMIT N`.\n",
    "   \n",
    "   6. __`AS` (named expressions):__\n",
    "   \n",
    "   You can specify new names for the expressions that you're selecting.  This is especially useful when you're using aggregate functions: \n",
    "```SQL\n",
    "SELECT SUM(salary) as 'Total Salary' FROM ...\n",
    "```\n",
    "  \n",
    "Finally two remarks: SQL is not case sensitive.  It is just common to use all-caps for the commands, and normal case for table / column names.  It is also whitespace and new-line insensitive, so you can format your commands on multiple lines as you please.\n",
    "\n",
    "\n",
    "Now for a bunch of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM Tasks;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM Assignments;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(1) as Count FROM Tasks WHERE status=0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT assignment_id, julianday(end_date) - julianday(start_date) \n",
    "    as duration FROM Assignments WHERE employee_id=2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT task_id, SUM((julianday(end_date) - julianday(start_date))*fraction_assignment) as Total_Worker_Days\n",
    "    FROM Assignments\n",
    "    GROUP BY task_id\n",
    "    ORDER BY Total_Worker_Days DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining tables: Joins and subqueries\n",
    "\n",
    "It's pretty boring to only use one table.  We can combine data from multiple tables in two main ways: joins and subqueries.\n",
    "\n",
    "**Nicknames:**\n",
    "\n",
    "There's a common bit of syntax involved in each: you can write\n",
    "\n",
    "    table_name name\n",
    "\n",
    "to say that table `table_name` is going to participate in this query with a \"nickname\" of name.  Then, to refer to one of its columns you write\n",
    "\n",
    "    name.column_name\n",
    "\n",
    "You could also have not given it the nickname, and then you could've written `table_name.column_name`... but that gets old fast.  Also, in some fancy examples the same table enters into a query twice -- at which point you need to name them.\n",
    "\n",
    "\n",
    "**Subqueries:**\n",
    "It turns out that both the expressions you were SELECTing __and__ the \"table\" you were selecting FROM can be subqueries (__and__ quantities in WHEREs)!  Let's just see it by example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT a.task_id, \n",
    "    (SELECT description \n",
    "     FROM Tasks t WHERE t.task_id=a.task_id) as description, \n",
    "    SUM((julianday(a.end_date) - julianday(a.start_date)) \n",
    "                * a.fraction_assignment) as Total_Worker_Days\n",
    "    FROM Assignments a\n",
    "    GROUP BY a.task_id\n",
    "    ORDER BY Total_Worker_Days DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT ad.task_id, ad.Total_Worker_Days \n",
    "    FROM (SELECT task_id, SUM((julianday(end_date) - julianday(start_date))*fraction_assignment) as Total_Worker_Days\n",
    "            FROM Assignments\n",
    "            GROUP BY task_id\n",
    "            ORDER BY Total_Worker_Days DESC) as ad\n",
    "    WHERE ad.task_id=(SELECT task_id FROM Tasks WHERE description LIKE 'Save%');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joins:**\n",
    "Many things that can be expressed by a subquery can be expressed more clearly (and will often run faster) as a `join`.   \n",
    "\n",
    "The basic idea of joins is to imagine taking the _product_ of your two tables: i.e. a row is now an ordered pair of a row of table 1 and a row of table 2.  You could -- of course -- do this and make it its own table, but the ability to write JOINs means that you don't have to waste the storage space and risk un-needed duplication of data.\n",
    "\n",
    "The join in the previous paragraph is called the full or outer join, because there are no conditions.  In practice, you'll want to take the subset of the product where some condition holds (i.e. \"join\" the two tables along a common id column).  This can be expressed pretty much exactly as I just said, in SQL:\n",
    "    \n",
    "```SQL\n",
    "       SELECT * FROM table1, table2\n",
    "                WHERE table1.column_name1 = table2.column_name2;\n",
    "```\n",
    "\n",
    "In SQL terms, this is called an `INNER JOIN`: You do not get a row in the result of the SELECT unless you can match a row in table 1 with a match in table 2 along the given columns.  There is alternate syntax for it:\n",
    "```SQL\n",
    "SELECT * FROM table1\n",
    "        INNER JOIN table2\n",
    "        ON table1.column_name1=table2.column_name2;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT a.*, t.* \n",
    "  FROM Assignments a, Tasks t \n",
    "  WHERE a.task_id=t.task_id \n",
    "    AND employee_id=2 \n",
    "    AND status=0\n",
    "  ORDER BY t.deadline;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More advanced joins\n",
    "\n",
    "Suppose we're gradually enrolling our employees in a pension plan.  We create a new table to store their pension plan choices and details.  Now we want to make a little reporting view that shows employees together with their pension choice -- in other words we want to \"join\" the Pension_Choices table to the Employees table, by lining up along employee_id.\n",
    "\n",
    "Great!  We just learned how to do that, right?  Not quite -- if an employee hasn't set up their pension yet, we don't want to drop them from the table, we just want to show NULL as the plan_name.\n",
    "\n",
    "This sort of situation -- where we want to allow the record in the *right hand* table to be not found -- is the reason for SQL's `LEFT OUTER JOIN`.  This requires explicit JOIN syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT e.employee_id, e.name, p.plan_name, p.funding_amount\n",
    "      FROM Employees e\n",
    "      LEFT OUTER JOIN Pension_Choices p\n",
    "      ON   e.employee_id = p.employee_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "SQL:\n",
    "- http://www.w3schools.com/sql/default.asp (and http://www.w3schools.com/sql/sql_quickref.asp)\n",
    "\n",
    "Relational databases and the normal forms:\n",
    "- http://en.wikipedia.org/wiki/Database_normalization#Normal_forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying tables\n",
    "\n",
    "There are also queries of the form\n",
    "\n",
    "        DELETE FROM table_name [WHERE condition]\n",
    "        \n",
    "        UPDATE table_name SET column_name1=value [WHERE condition];\n",
    "\n",
    "that delete / modify rows as you might expect.  You can combine both of these with inner queries (and in Postgres, you can combine UPDATE with JOINs) for interesting effects.\n",
    "\n",
    "To modify columns there are commands of the form\n",
    "\n",
    "        ALTER TABLE table_name DROP COLUMN column_name;        \n",
    "        ALTER TABLE table_name ADD COLUMN column_name column_type;\n",
    "        \n",
    "for instance\n",
    "\n",
    "        ALTER TABLE Employees DROP COLUMN phone_number;\n",
    "        ALTER TABLE Employees ADD COLUMN salary INT;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use SQL and don't have too much data (less than a few gigs at a time), you can use `sqlite3`.  It's already installed on DigitalOcean, is easy to use, and has excellent (and lenient) support for importing and exporting CSV.\n",
    "\n",
    "Look on [the wiki](https://sites.google.com/a/thedataincubator.com/the-data-incubator-wiki/course-information-and-logistics/getting-started/setup) for instructions on setup and installation of the mathematical function extension.\n",
    "\n",
    "### Exporting to CSV\n",
    "For instance, to export the above tables to CSV:\n",
    ">           \n",
    "    .mode csv\n",
    "    .headers on\n",
    "    .output tasks.csv\n",
    "    SELECT * FROM Tasks;\n",
    "    .output employees.csv\n",
    "    SELECT * FROM Employees;\n",
    "    .output assignments.csv\n",
    "    SELECT * FROM Assignments;\n",
    "    \n",
    "    \n",
    "### Importing CSV\n",
    "Conversely if we already had these CSV files, we could set up our tables just by doing\n",
    ">    \n",
    "    .mode csv\n",
    "    .import tasks.csv Tasks\n",
    "    .import employees.csv Employees\n",
    "    .import assignments.csv Assignments\n",
    "    \n",
    "Note that we didn't have to specify the schema ahead of time (though we might want to).  SQLite will take column names from the CSV header line, and will make all fields have type TEXT and be NULLable.\n",
    "\n",
    "### Listing tables / table schema\n",
    ">      \n",
    "    .tables\n",
    "    .schema Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgresql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick facts\n",
    "\n",
    "**Running a server:**\n",
    "- On your DO droplets, once you install postgres it will automatically set up the server to run in the background on startup.  The actual database files are hidden somewhere inside of `/var`.\n",
    "\n",
    "- Alternatively, you can run a \"local\" postgres server whose data lies in a specific directory and on a specific port:\n",
    "\n",
    ">         initdb dir/\n",
    ">         postgres [-p 8765] -D dir/\n",
    "\n",
    "(You may want to specify a custom port that is not my made up 8765 -- the default is __5432__.)\n",
    "\n",
    "**Quick start guide:**\n",
    "After the above is done and you're running a postgres server locally, just do\n",
    "\n",
    "         createdb [-p 8765] my_db_name\n",
    "\n",
    "to create a new database.  When you're done with it\n",
    "\n",
    "        dropdb --if-exists [-p 8765] my_db_name\n",
    "\n",
    "To get a command-line console for running commands against your database, just run\n",
    "\n",
    "        psql [-p 8765] my_db_name\n",
    "\n",
    "(If it's running on the standard port, you don't need to specify the `-p 8765`.)\n",
    "\n",
    "By default, this will all use a user named for your Unix username -- for the system Postgres install, this user (\"role\" in Postgres terms) was created when setting up your droplet.\n",
    "\n",
    "**Postgres-specific basics:**\n",
    "To list all tables or find out info about a table do:\n",
    "\n",
    "        \\d\n",
    "        \\d table_name\n",
    "\n",
    "**Importing (CSV) data:**\n",
    "If your data is pretty clean, the `\\COPY` special command should do it\n",
    "\n",
    "        \\COPY table_name FROM 'path/to/my/file.csv' WITH CSV DELIMITER ',' NULL AS '';\n",
    "        \n",
    "**Exporting (CSV) data:** You can use `\\COPY` in both directions\n",
    "        \n",
    "        \\COPY table_name TO 'path/to/my/file.csv' WITH DELIMITER ',' CSV HEADER;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL integration in Python\n",
    "\n",
    "The most common usage mode for you will be importing / exporting to CSV, or working interactively through `sqlite3` / `psql`.\n",
    "\n",
    "You can also interact between Python and SQL.  We've been doing it throughout this whole notebook using the `%sql` magic.  There are also libraries for doing it less magically\n",
    "\n",
    " - `psycopg2` for communicating with Postgres\n",
    " - `sqlalchemy` for communicating with most anything, and some fancy ORM tricks\n",
    "\n",
    "There's nothing too difficult about either, so we'll just learn them by example in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to use?\n",
    "-------------\n",
    "\n",
    "**SQLite:**\n",
    "-------------\n",
    "_In short:_ Use this if your data is smallish (several GB max) and you want to do some SQL-based processing, _and_ if you know ahead of time that you will not need to do operations in parallel. Probably the preferred choice for mini-projects.\n",
    "\n",
    "**Pros**\n",
    " - It operates on a single flat file on disk, which you can treat like any other file -- copy, scp, rename, etc.\n",
    " - It is a single small executable, comes pre-installed on many things, and doesn't require running a server.\n",
    " - Extensible (by C code).  It's open source and is small enough to be easy to modify it / write extension modules in C.\n",
    " - It has a quick and permissive (of errors) CSV loader, and doesn't stress correctness of schemas.\n",
    "\n",
    "**Cons**\n",
    " - It lacks some useful \"advanced\" features (in particular, JOINs in UPDATEs have to be faked via subqueries and/or temporary tables).  The workarounds are generally slower than the \"right\" approach would be.\n",
    " - It's not multi-user.  If you're running a long-running computation, you can't do anything else with it.\n",
    "\n",
    "**Postgresql:**\n",
    "----------------\n",
    "_In short:_  The premier free \"real\" RDBMS (i.e. SQL-type database).  Use it if you need SQL semantics and have more than several GB of data or need multi-user / multi-process access.  \n",
    "[Exception: If you have lots of data and do lots of easily parallelizable tasks, and will be running either on a cluster on a machine that fits your data in RAM, you may be better off with a specialized SQL-like system: e.g. Hive, Shark, MemSQL (commercial).]\n",
    "\n",
    "**Pros**\n",
    " - Excellent support of SQL features, good performance and `\"EXPLAIN ...\"` for seeing query plans and tweaking performance.\n",
    " - Several nice SQL extensions: JSON-column support, and `Postgis` for GIS-based geographical data types and queries.\n",
    " - Extensible (in a variety of languages, _including Python_).\n",
    " \n",
    "**Cons**\n",
    " - You have to run a server.\n",
    " - The CSV loader is quick on correct data, but simply doesn't except malformed data.  Does stress correctness of schemas.\n",
    " \n",
    " \n",
    " **Other options:**\n",
    " ------------------\n",
    " - *If your data is small enough to fit _comfortably_ into memory:* \n",
    " \n",
    "   You can get by with something like `pandas` which allows for much quicker visualization and has better built in statistical support.  (There is also support for running SQL queries against pandas.  It uses sqlite behind the scenes -- beware that doing this has a performance cost over just using sqlite originally.)\n",
    " - *If you have have *lots* of data (i.e. more than any one machine you have can store on disk) or if you want to perform a task that calls for cross-machine parallelization (e.g. it makes sense to run on Hadoop):*\n",
    " \n",
    "   There are specialized SQL-like databases made for this, notably `Hive` which is built on top of Hadoop.  (Also: `Shark` and `Impala` which do the same thing with different size and performance characteristics.)\n",
    " - *If your data set is large, but you happen to have a _very_ beefy sever (or a cluster of them) with enough RAM to fit it all:*\n",
    " \n",
    "  There are SQL-like databases optimized for in-memory use, including `MemSQL` and also modes of `Shark` and `Impala` mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "Exercise 2 may be too much of a scraping, rather than SQL, problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "1. Create a SQL database for storing [CBSAs](http://www.census.gov/geo/reference/gtc/gtc_cbsa.html) ([read more](http://en.wikipedia.org/wiki/Core_Based_Statistical_Area)) together with the following basic info: \n",
    "    - population (2012); \n",
    "    - area; \n",
    "    - type;\n",
    "    - states;\n",
    "\n",
    "  Clearly describe how many tables you will use, what the keys are, etc.\n",
    "\n",
    "2. Write a script to populate your database by using some combination of [the OMB deliniation files](http://www.census.gov/population/metro/data/def.html) and [Wikipedia's list of CBSAs](http://en.wikipedia.org/wiki/List_of_Core_Based_Statistical_Areas) and the links therein.  If those links don't work, use `small_data/List1.xls` and `List_of_Core_Based_Statistical_Areas`.\n",
    "\n",
    "  *Hint:* the OMB files are in Excel.  You can use Python's `xlrd` library to read the data in and `csv` to write out the data to a csv.  Wikipedia's data is an HTML table.  Try using `pandas.read_html` function to save the data as a csv.  You can then load the data in Sqlite.  What's the common key?  You might need to massage the data in Python if the keys do not align.\n",
    "\n",
    "3. Perform a few joins in SQL.  Give the total urban population for 2010 and 2012 by state.  Get the total central vs. outlying population (see the OMB datafile for the definition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets\n",
    "1. You want to find all comments made by a user by joining the users and comments tables. What kind of join should you use?\n",
    "1. What are the benefits/drawbacks of doing data analysis in SQL versus Python?\n",
    "1. Is a primary key necessary to perform a join?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoiler space..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the sort of answer I'd get for #1:\n",
    "```SQL\n",
    "CREATE TABLE CBSA\n",
    "    ( cbsa_id INTEGER NOT NULL\n",
    "    , cbsa_name TEXT NOT NULL\n",
    "    , cbsa_type VARCHAR(4) NOT NULL\n",
    "    , population INTEGER NOT NULL\n",
    "    , area INTEGER NOT NULL\n",
    "    , PRIMARY KEY cbsa_id\n",
    "    );\n",
    "CREATE TABLE CBSA_States\n",
    "    ( cbsa_id INTEGER NOT NULL\n",
    "    , state_code  CHAR(2) NOT NULL\n",
    "    , PRIMARY KEY (cbsa_id, state)\n",
    "    );\n",
    "```\n",
    "\n",
    "While for #2, we can first introduce a new helpful tool.  The data came to us in XLS form, so how do we load Excel files into Python?  The easiest way -- if you have Excel installed and don't care about automating the process -- is to export the XLS to CSV, and load the CSV.\n",
    "\n",
    "An alternate way is to use a library called __xlrd__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We're going to have to read an Excel file!\n",
    "\n",
    "import xlrd\n",
    "import urllib2\n",
    "\n",
    "wb = xlrd.open_workbook(file_contents = urllib2.urlopen(\"http://www.census.gov/population/metro/files/lists/2013/List1.xls\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wb.sheet_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sheet1 = wb.sheet_by_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print sheet1.row(i) \n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Does this file look like it would make for a well-designed (\"normalized\") table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = [sheet1.row(i) for i in range(3, sheet1.nrows-3)]\n",
    "data = [(r[0].value, r[4].value, r[8].value, r[3].value) for r in rows]\n",
    "print data[0], data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "long_to_short_type = {\n",
    "    'Metropolitan Statistical Area' : 'MSA',\n",
    "    'Micropolitan Statistical Area' : 'mSA'\n",
    "}\n",
    "def reverse_dict(dict):\n",
    "    return {v:k for k, v in dict.items()}\n",
    "short_to_long_type = reverse_dict(long_to_short_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import us\n",
    "long_to_short_state = us.states.mapping('name', 'abbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sets import Set\n",
    "cbsa_by_id = {}\n",
    "cbsa_id_by_name = {}\n",
    "# Build the ids/names/types/states from the OMB data.\n",
    "for x in data:\n",
    "    (cbsa_id_str, type_long, state_long, cbsa_name) = x\n",
    "    cbsa_id = int(cbsa_id_str)\n",
    "    if cbsa_id not in cbsa_by_id:\n",
    "        cbsa_by_id[cbsa_id] = { \n",
    "                'cbsa_id': cbsa_id, \n",
    "               'cbsa_name': cbsa_name, \n",
    "               'cbsa_type': long_to_short_type[type_long], \n",
    "               'population' : None,\n",
    "               'area' : None,\n",
    "               'states' :  Set([])\n",
    "        }\n",
    "        cbsa_id_by_name[cbsa_name] = cbsa_id\n",
    "    cbsa_by_id[cbsa_id]['states'].add( long_to_short_state[state_long]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cbsa_by_id.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def guess_url(cbsa):\n",
    "    url_base=\"http://en.wikipedia.org/wiki/\"\n",
    "    url_rest = (cbsa['cbsa_name']+'_'+short_to_long_type[cbsa['cbsa_type']]).replace(' ', '_')\n",
    "    return  url_base + url_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def add_info(cbsa):\n",
    "    url=guess_url(cbsa)\n",
    "    print url\n",
    "    soup = BeautifulSoup(urllib2.urlopen(url))\n",
    "    # Exercise: Fill this in!\n",
    "    #\n",
    "    # Perhaps look for any tag that has text, and that this text is (after suitable stripping) = \"Population\" (resp., \"Area\")\n",
    "    # and then search for the next tag which says \"Total\", and finally take the next td!\n",
    "    # \n",
    "    # This might be a good place to talk about regular expressions...\n",
    "    #\n",
    "    return cbsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load wiki table using \n",
    "\n",
    "import pandas as pd\n",
    "tables = pd.read_html(\"http://en.wikipedia.org/wiki/List_of_Core_Based_Statistical_Areas\", header=0)\n",
    "cbsa_table = tables[0]\n",
    "# cbsa_table.to_csv(\"wiki_table.csv\")  # saves table as csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
