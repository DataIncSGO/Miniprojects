{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Large  / High Dimensional Datasets\n",
    "\n",
    "## Unique Concerns with Large Data\n",
    "\n",
    "When attempting to visualize large or high-dimensional datasets, our concerns are twofold:\n",
    "\n",
    "1. (you-facing) Effectively computing the visualization in code.\n",
    "2. (end-user-facing) Effectively conveying the key points to viewers.\n",
    "\n",
    "Both of these concerns stem from the same issue: the dataset simply contains tons of information, and it's your job to separate the signal from the noise both for reasons of computational tractability and visualization efficacy.\n",
    "\n",
    "There are a couple ways ways to reduce the amount of data your visualization relies on. We'll discuss\n",
    "\n",
    "- preprocessing our dataset (eg subsampling, PCA)\n",
    "- filtering or querying in realtime for subsets/groups of the data (assuming you have a powerful enough backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA And Dimensionality Reduction\n",
    "\n",
    "As a technique for dimensionality reduction, [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) is very useful (see Module 3 for more info on PCA). One way we can use this to our advantage is by mapping high-dimensional data into 2D space for quick and easy visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/glemaitre/UnbalancedDataset/blob/master/notebook/Notebook_UnbalancedDataset.ipynb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "# Generate some data\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=10)\n",
    "\n",
    "# Instanciate a PCA object for the sake of easy visualisation\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# Fit and transform x to visualise inside a 2D feature space\n",
    "x_vis = pca.fit_transform(X)\n",
    "\n",
    "# Plot the original data\n",
    "# Plot the two classes\n",
    "palette = sns.color_palette()\n",
    "plt.scatter(x_vis[y==0, 0], x_vis[y==0, 1], label=\"Class #0\", alpha=0.5, \n",
    "            edgecolor='#262626', facecolor=palette[0], linewidth=0.15)\n",
    "plt.scatter(x_vis[y==1, 0], x_vis[y==1, 1], label=\"Class #1\", alpha=0.5, \n",
    "            edgecolor='#262626', facecolor=palette[2], linewidth=0.15)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fairly contrived example. In general, further explanation is likely needed if you decide to use PCA or other dimensionality-reduction techniques to visualize some subset or aggregate of your data. For a more interesting application, check out this visualization of word2vec feature space clustering ([github](http://github.com/ronxin/wevi)) ([online demo](http://bit.ly/wevi-online)) (from [this paper](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf)).\n",
    "\n",
    "Two other dimensionality reduction alternatives: [MDS](https://en.wikipedia.org/wiki/Multidimensional_scaling) and [Denoising Autoencoders](https://en.wikipedia.org/wiki/Autoencoder#Denoising_Autoencoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandas to filter and downsample for visualization\n",
    "\n",
    "### High-level considerations\n",
    "\n",
    "When visualizing data, it usually makes litte sense to try to convey gigabytes' (or more) worth of information to an end-user (unless your end-user is [Adrian Veidt](http://i46.tinypic.com/1qqtmw.jpg)). Plus, this slows down computation on the backend. A little pre-processing goes a long way toward addressing both issues 1) and 2) from the beginning of this notebook.\n",
    "\n",
    "Some considerations:\n",
    "- When downsampling timeseries data, make sure that your sample is sufficiently representative to include time-based effects. Can you think of some examples?\n",
    "- Consider visualizing a deliberate subset (not via sampling) of the data. Using some subset, can you successfully reinforce the message you're trying to convey? Do you want to provide another visualization to compare/contrast with the rest of the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips and tricks from the field in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "df = pd.read_csv('small_data/fha_by_tract.csv', names=names)  ## Loading a CSV file, without a header (so we have to provide field names)\n",
    "\n",
    "\"\"\"\n",
    "Borrowed from the Pandas notebook (see module 1 for more info).\n",
    "- Notice how the distribution of the two histograms is the same. \n",
    "- If you don't care about raw counts, sampling speeds up visualization\n",
    "while conveying the exact same information.\n",
    "\"\"\"\n",
    "df['PCT_AMT_FHA'].hist(bins=50, alpha=0.5)\n",
    "df['PCT_AMT_FHA'].sample(frac=.5).hist(bins=50, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the visualization miniproject, we provide a pre-cleaned ~80MB dataset. We did a fair bit of processing from a week's worth of raw CSV bus time data (about 300MB each). If you'd like to try your hand at downsampling yourself, we've provided some of that code here, highlighting the techniques we used to make the data Heroku-sized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# the raw MTA data actually needs a slightly different parser, if you copy this for your own uses\n",
    "date_parser=lambda u: pd.datetime.strptime(u, \"%Y-%m-%d %H:%M:%S\")\n",
    "CSV_FILENAMES=[\"small_data/bustime_micro.csv\"]\n",
    "archive = pd.concat([pd.read_csv(fname,  \n",
    "                                 parse_dates=True,\n",
    "                                 date_parser=date_parser,\n",
    "                                 index_col=0) for fname in CSV_FILENAMES])\n",
    "#query and clean\n",
    "live_archive = archive[archive.block_assigned > 0] # \"assigned\" to a route\n",
    "\n",
    "#throw away trips with <15 reports, probably not real\n",
    "good_trips_only = live_archive.groupby(live_archive.trip_id).filter(lambda group: len(group) > 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## join to trip data and filter to manhattan only (ie look at a subset)\n",
    "## Note the \"reset_index\" to deal with merging index complications.\n",
    "# good_trips_only.reset_index(inplace=True)\n",
    "# merged = good_trips_only.merge('trips.txt', on='trip_id', left_index=True)\n",
    "# merged.sort('timestamp', inplace=True)\n",
    "# merged.set_index('timestamp')\n",
    "# manhattan_only = merged[merged.route_id.str.contains(\"^M\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_intervals_for_archive():\n",
    "    intervals = [(datetime(2015, 9, d, h, m),\n",
    "                   datetime(2015, 9, d, h, m + 2))\n",
    "                for h in range(0, 24)\n",
    "                for m in range(0, 60, 15)\n",
    "                for d in range(13, 21)]\n",
    "    return intervals\n",
    "\n",
    "#exercise: any better solution for this?\n",
    "subsampled_dataset = pd.concat([good_trips_only[lower:upper]\n",
    "                                for lower, upper in get_intervals_for_archive()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still load 7 days' worth of data into memory and preprocess without too much trouble. What we have a dataset that won't fit into RAM?\n",
    "\n",
    "Pandas has a solution for that: `skiprows`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "f = \"small_data/fha_by_tract.csv\"\n",
    "\n",
    "with open(f) as in_file:\n",
    "    num_lines = sum(1 for l in in_file)\n",
    "\n",
    "n = 10\n",
    "# make sure line 0 is not included if you have a header!\n",
    "skip_list = [x for x in range(1, num_lines) if x % n != 0]\n",
    "\n",
    "# Read the data\n",
    "%time data1 = pd.read_csv(f, skiprows=skip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a generator will also work. note it is slower.\n",
    "skip_gen = (x for x in range(1, num_lines) if x % n != 0)\n",
    "\n",
    "%time data2 = pd.read_csv(f, skiprows=skip_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the web - live-query a backend\n",
    "\n",
    "One of the more powerful aspects of web-based/interactive visualizations is the ability to query and render data in realtime. This allows us to, in some contexts, live-query for some subset of the data and effectively downsample this way.\n",
    "\n",
    "- On the web, we always have to think about server -> client bandwidth considerations. \n",
    "- Remember, end-users don't necessarily have modern, powerful computers.\n",
    "- Always assume the worst (the visualization equivalent of the UI/UX adage \"assume the user is drunk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it in Python: the Bokeh Server\n",
    "One of Bokeh's most powerful abstractions is the Bokeh Server, which lets you run a visualization server in the background that can live-query, process, filter, and compute visualizations using your more powerful backend infrastructure. For example, you might wish to let a user live-query site log data, or hit information, over varying timespans.\n",
    "\n",
    "Check out the [Bokeh Gallery](http://bokeh.pydata.org/en/latest/docs/gallery.html#gallery) for some inspiration, and the [Bokeh Docs](http://bokeh.pydata.org/en/0.11.0/docs/user_guide/server.html) explain how the update functionality/synchronization works.\n",
    "\n",
    "Under the hood, Bokeh is doing the same AJAX magic as the D3 example below, but you don't have to concern yourself as much with the details.\n",
    "\n",
    "If you want to play around with the Bokeh server on Heroku, it does work! [With some fiddling](https://github.com/birdsarah/bokeh-server-heroku)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it on the Web: D3.js and AJAX calls.\n",
    "\n",
    "AJAX (advanced Javascript and XML) is a powerful web paradigm that allows for asynchronous calls between a backend server and a frontend browser. \n",
    "\n",
    "![Here, have a fancy diagram](http://www.openajax.org/images/AjaxContinuousExperience.gif)\n",
    "\n",
    "\n",
    "For example, if you're running code in Jupyter notebooks, you're using AJAX to \n",
    "\n",
    "1. send your code to the backend Jupyter server.\n",
    "2. receive the output after the Jupyter server executes your code.\n",
    "\n",
    "In the context of visualization, this allows us to live-query some subset of view of the data, process appropriately on the backend, and simply render on the frontend. Here's an example for the bus heatmap miniproject:\n",
    "\n",
    "### JS (frontend)\n",
    "```javascript\n",
    "//NOTE: requires the underscore.js library\n",
    "//NOTE: this code will slam the backend with requests because it has no delay.\n",
    "// It's simply meant to showcase some asynchronous code that loops over small intervals.\n",
    "// We could use the setTimeout function in javascript to delay this code.\n",
    "\n",
    "_.range(24).foreach(function(hours){\n",
    "    _.range(0, 60, 15).foreach(function(minutes){\n",
    "        // use AJAX to query bus positions at \n",
    "        var swipes = d3.csv(\"/positions_at_time.csv?hours=\" + hours + '&minutes=' + minutes)\n",
    "        .get(function(error, rows){\n",
    "          var points = rows.map(function(r){\n",
    "            return  [\n",
    "              +r.latitude,\n",
    "              +r.longitude,\n",
    "            ];\n",
    "        });\n",
    "    });\n",
    "});\n",
    "```\n",
    "\n",
    "### Python (backend)\n",
    "```python\n",
    "# NOTE: code here written for Flask framework\n",
    "@app.route('/positions_at_time.csv')\n",
    "def positions_at_time():\n",
    "  hours = int(request.args.get('hours', 12))\n",
    "  minutes = int(request.args.get('minutes', 30))\n",
    "  \n",
    "  #could easily make this parameter modifiable\n",
    "  date_for_data = datetime.date(2015, 9, 17)\n",
    "  \n",
    "  time_for_data = datetime.time(hours, minutes)\n",
    "  \n",
    "  datetime_for_data = datetime.datetime.combine(date_for_data, time_for_data)\n",
    "  \n",
    "  #2 minute window\n",
    "  lower_bound = datetime_for_data - datetime.timedelta(seconds=60)\n",
    "  upper_bound = datetime_for_data + datetime.timedelta(seconds=60)\n",
    "\n",
    "  #JSON would work too, but CSV is actually better for D3\n",
    "  return archive[lower_bound:upper_bound].to_csv()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
