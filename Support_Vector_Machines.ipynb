{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM\n",
    "\n",
    "The canonical Support Vector Machine is the linear one.  Assume we have two groups labeled by $y = \\pm 1$.  Then we are trying to find the line $\\beta$ such that $X \\beta + \\beta_0$ maximally separates the points in our two classes:\n",
    "\n",
    "![SVM Diagram from Hastie et al's The Elements of Statistical Learning](images/svm3.png)\n",
    "\n",
    "If the two classes can be separated by a linear hyperplane (picture on the left), we want to maximize the **margin** $M$ of the **boundary region**.  A little bit of math can show us that this is the same as solving the minimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\beta_0} \\|\\beta\\| \\\\\n",
    "\\mbox{subject to } y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge 1 \\quad \\mbox{for } j = 1,\\ldots,N\n",
    "$$\n",
    "\n",
    "The picture and the equation are equivalent: in the picture we are setting the margin to be $M$ and finding the largest margin possible.  In the equation, we are setting the margin to be $1$ and finding the smallest $\\beta$ that will make that true.  So $\\beta$ and $M$ are related through $\\| \\beta \\| = \\frac{1}{M}$.  If the two classes cannot be separated (picture on the right), we will have to add forgiveness terms $\\xi$,\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\beta_0} \\|\\beta\\| \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    " y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge (1-\\xi_j) & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\xi_j \\ge 0 & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\sum_j \\xi_j \\le C\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "for some constant $C$.  The constant $C$ is an important trade-off.  It corresponds to the total \"forgiveness budget\" (see the last constraint).  The larger $C$, the forgiveness we have and the wider the margin $M$ can be.  We can rewrite the constrained optimization problem as the primal Lagrangian function with Lagrange multipliers $\\alpha_j \\ge 0$, $\\mu_j \\ge 0$, and $\\gamma \\ge 0$,  for each of our three constraints:\n",
    "\n",
    "$$ L_P(\\gamma) = \\min_{\\beta, \\beta_0, \\xi} \\max_{\\alpha, \\mu} \\frac{1}{2} \\| \\beta \\|^2 - \\sum_j \\alpha_j \\left[y_j (X_{j \\cdot} \\cdot \\beta + \\beta_0 - (1-\\xi_j)\\right] - \\sum_j \\mu_j \\xi_j  + \\gamma \\sum_j \\xi_j$$\n",
    "\n",
    "There is a one-to-one correspondence between $\\gamma$ and $C$.  By taking first order conditions, first-order conditions, the dual Lagrangian problem can be formulated as\n",
    "\n",
    "$$\n",
    "L_D(\\gamma) = \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_{j, j'} \\alpha_j \\alpha_{j'} y_j y_{j'} X_{j \\cdot} \\cdot X_{j' \\cdot} \\,. \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    "0 = \\sum_j \\alpha_j y_j \\\\\n",
    "0 \\le \\alpha_j \\le \\gamma & \\mbox{for } j = 1,\\ldots,N\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "This is now a reasonably straightforward quadratic programming problem, solved via [Sequential Minimization Optimization](https://en.wikipedia.org/wiki/Sequential_minimal_optimization).  Once we have solved this problem for $\\alpha$, we can easily work out the coefficients from\n",
    "\n",
    "$$ \\beta = \\sum_j \\alpha_j y_j X_{j \\cdot} $$\n",
    "\n",
    "**Key takeaways**:\n",
    "1. Critically, only points inside the margin or on the wrong side of the margin ($j$ for which $\\xi_j > 0$) affect the SVM (see the picture).  This is intuitively clear from the picture.  In the dual form, this is because $\\alpha_j$ is the Lagrangian constraint corresponding to $y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge (1-\\xi_j)$ and Complementary Slackness shows tells us that $\\alpha_j > 0$ is non-zero only when the constraint is binding ($y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) = (1-\\xi_j)$), i.e. we're in the boundary region.  This is meaning the **Support Vector** in \"SVM\": only the vectors in the boundary-the **Support Vectors**-contribute to the solution.\n",
    "1. $C$ or $\\gamma$ give a trade-off between the amount of forgiveness and the size of the margin or boundary region.  Hence, it controls how many points affect the SVM (based on the distance from the boundary).\n",
    "\n",
    "Below, we plot out a simple two-class linear SVM on some synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "from sklearn import svm\n",
    "\n",
    "x_min = -.5\n",
    "x_max = 1.\n",
    "\n",
    "# we only take the first two features for visualization\n",
    "X = np.random.uniform(x_min, x_max, size=[1000,2])\n",
    "y = np.linalg.norm(X, axis=1) < .8\n",
    "\n",
    "#set our error margin\n",
    "C=1\n",
    "# linear kernel classifier\n",
    "clf = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "\n",
    "# create a mesh to plot for clf values\n",
    "h = .005  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(x_min, x_max, h))\n",
    "\n",
    "# predicted values\n",
    "zz = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, cmap=plt.cm.OrRd)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.OrRd)\n",
    "plt.xlim([x_min,x_max])\n",
    "plt.ylim([x_min,x_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear SVM\n",
    "\n",
    "What if we don't believe that our data can be cleanly split by a linear hyperplane?  The common way to incorporate non-linear features is to have a non-linear function $h(X_{j\\cdot})$ (possibly to a higher-dimensional feature space with dimension $p'$ where $p' \\ge p$) and to train on that space.  One intuition is that there's a higher-dimensional space in which the data is has a linear separation and $h$ gives a non-linear mapping into that space.\n",
    "\n",
    "## Kernel Trick\n",
    "\n",
    "The **Kernel Trick** in SVM tells us that rather than directly computing the (potentially very large) vectors $h(X_{j \\cdot})$, we can just modify the Kernel.  If we use the transformed data $h(X_{j \\cdot})$, the dual Lagrangian would be\n",
    "\n",
    "$$ \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_j \\sum_{j'} \\alpha_j \\alpha_{j'} y_j y_{j'} h(X_{j \\cdot}) \\cdot h(X_{j' \\cdot}) $$\n",
    "\n",
    "We can rewrite\n",
    "\n",
    "$$h(X_{j \\cdot}) \\cdot h(X_{j' \\cdot})  = K(X_{j \\cdot}, X_{j' \\cdot})$$ \n",
    "\n",
    "for some non-linear Kernel $K$.  Our problem then becomes,\n",
    "\n",
    "$$ \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_j \\sum_{j'} \\alpha_j \\alpha_{j'} y_j y_{j'} K(X_{j \\cdot}, X_{j' \\cdot}) $$\n",
    "\n",
    "There's a one-to-one correspondence between Kernel functions and functions $h$ (although $h$'s range may be infinite dimensional).  Some common Kernels include\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Kernel</th>\n",
    "<th>$K(x,x')$</th>\n",
    "<th>Scikit `kernel` parameter</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Linear Kernel</td>\n",
    "<td>$x \\cdot x'$</td>\n",
    "<td>`kernel='linear'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>$d$-th Degree Polynomial</td>\n",
    "<td>$(r + c x \\cdot x')^d$</td>\n",
    "<td>`kernel='poly'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Radial Kernel</td>\n",
    "<td>$ \\exp(- c \\|x - x' \\|^2) $</td>\n",
    "<td>`kernel='rbf'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Neural Network Kernel</td>\n",
    "<td>$\\tanh(c x \\cdot x' + r)$</td>\n",
    "<td>`kernel='sigmoid'`</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The benefit of using a Kernel is that we don't have to compute a very high-dimensional (possibly infinite-dimensional) $h$.  All that complexity is just wrapped into the kernel $K$.\n",
    "\n",
    "For more information on which parameters to pass the kernel, checkout [the Scikit documentation](http://scikit-learn.org/stable/modules/svm.html#svm-kernels).\n",
    "\n",
    "**Questions**:\n",
    "1. Given a trained SVM model, how would you make predictions?  Are you able to compute the values for $\\beta$?.\n",
    "1. How does the memory and time scale with $n$, the number of samples, and $p$, the number of features for each kernel $K$?\n",
    "1. How might we reduce the computation time if one class contains the majority of samples?\n",
    "1. What happens if each of the features is on a very different scale?  How could you correct for this?\n",
    "1. Which of these kernels would benefit from subtracting the mean of each feature fom the data?\n",
    "1. Modify the above example code to use non-linear kernels and examine what the decision boundaries look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class SVM\n",
    "\n",
    "To do multi-class SVM, recall that there are generally two strategies, One-versus-One, and One-versus-All.  [Scikit uses One-versus-One (sometimes called All-versus-All strategy)](http://scikit-learn.org/stable/modules/multiclass.html).  If $f_{ij}$ is the classifier value where $i$ is a positive label ($y=1$) and $j$ is a negative label ($y=-1$), then we choose\n",
    "$$ \\mbox{argmax}_i \\sum_{j \\neq i} f_{ij}$$\n",
    "\n",
    "**Question**:\n",
    "1. Why would you choose One-versus-One or One-versus-All?  (One-versus-All requires more memory and everyone claims SVM is super-linear in memory so we're willing to run more simulations).\n",
    "1. Can you improve the accuracy by taking some of the steps outlined above?\n",
    "1. Modify the above code to do the full multi-class SVM (`SVC` automatically does multiple classes, you just have to feed in all the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample code to test SVM Classification\n",
    "\n",
    "from sklearn import svm, linear_model, cross_validation, metrics, random_projection\n",
    "import pandas as pd\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(len(y), n_iter=3, test_size=0.25, random_state=42)\n",
    "C = 1.0\n",
    "\n",
    "models = [\n",
    "    (\"Logistic Regression\", linear_model.LogisticRegression()),\n",
    "    (\"Linear Kernel\", svm.SVC(kernel='linear', C=C)),\n",
    "    (\"RBF\", svm.SVC(kernel='rbf', gamma=1e-1, C=C)),\n",
    "    (\"Polynomial\", svm.SVC(kernel='poly', degree=2))\n",
    "]\n",
    "\n",
    "pd.DataFrame([\n",
    "    (name, cross_validation.cross_val_score(clf, X, y, scoring='accuracy', cv=cv).mean()) for name, clf in models\n",
    "], columns=[\"Model\", \"MSE\"]).plot(x=\"Model\", y=\"MSE\", kind=\"bar\", title=\"Accuracy of various models\", ylim=[.6,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating Kernels\n",
    "\n",
    "The problem with the Kernel trick is that the memory required for $K$ is $O(n^2)$ and this can be both slow to compute over and expensive memory-wise.  Instead, we might choose to transform our data via an approximation $\\tilde h$ of the transformation function $h$.  For example, Scikit's `kernel_approximation.Nystroem` provides transformations that approximate each of the non-linear kernels.  Once you have the transformed features, you can pass them to `svm.LinearSVC` class to compute a linear support vector machine.\n",
    "\n",
    "For more information [Kernel Approximation](http://scikit-learn.org/stable/modules/kernel_approximation.html)\n",
    "\n",
    "**Exercise**: Load the MNIST Digits dataset.  This is a dataset of handwritten digits 0 - 9 that is used as a canonical training example:\n",
    "\n",
    "```\n",
    "digits = datasets.fetch_mldata('mnist-original')\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "```\n",
    "\n",
    "1. Try to build build a classifier using `svm.SVC` with a non-linear Kernel.  It will just take a long time ...\n",
    "1. Use `kernel_approximation.Nystroem` and `svm.LinearSVC` chained together via `pipeline.Pipeline` to build a tractable classifier.  Use grid_search to find optimal parameters.  *Hint:* to build up the learner, first restrict the number of classes and the number of examples and then slowly lift those restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression\n",
    "\n",
    "Support vector machines are also sometimes used to solve regression problems.  In regression, $y$ takes on real values instead of just $\\pm 1$.  In the classification case, we penalized the term for being either inside or on the *wrong* side of the margin (**Hinge Loss**).  In regression, we want to penalize for being too far away from the predicted value, regardless of whether you are above or below the margin  (**Well Loss**).  Below, we give a plot of the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot of the two loss functions\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def hinge_loss(x):\n",
    "    return np.maximum(0, x+1.)\n",
    "\n",
    "def well_loss(x):\n",
    "    return np.maximum(0, np.abs(x)-1.)\n",
    "\n",
    "x = np.arange(-4, 4, .1)\n",
    "plt.plot(x, hinge_loss(x), label=\"Hinge Loss\")\n",
    "plt.plot(x, well_loss(x), label=\"Well Loss\")\n",
    "plt.ylim([-1, 5])\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematics of SVM Loss\n",
    "\n",
    "$$\n",
    "L_P = \\min_{\\beta, \\beta_0} \\|\\beta\\| \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    " y_j - X_{j\\cdot} \\cdot \\beta - \\beta_0 \\le \\epsilon + \\xi_j & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " X_{j\\cdot} \\cdot \\beta + \\beta_0 - y_j \\le \\epsilon + \\xi_j^* & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\xi_j \\ge 0 & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\xi_j^* \\ge 0 & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\sum_j \\xi_j + \\sum_j \\xi_j^* \\le C\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "We can perform a similar calculations to incorporate the Kernel for a new feature map $h$ and real a dual quadratic programming problem.  [Here's a simple article that gives some of the details](http://alex.smola.org/papers/2003/SmoSch03b.pdf).  \n",
    "\n",
    "$$\n",
    "L_D(\\gamma) = \\max_{\\alpha} \\sum_j y_j (\\alpha_j - \\alpha_j^*) - \\epsilon \\sum_j (\\alpha_j - \\alpha_j^*) - \\frac{1}{2} \\sum_{j, j'} (\\alpha_j - \\alpha_j^*) (\\alpha_{j'} - \\alpha_{j'}^*) K(X_{j \\cdot}, X_{j' \\cdot}) \\,. \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    "0 = \\sum_j (\\alpha_j -\\alpha_j^*) \\\\\n",
    "0 \\le \\alpha_j \\le \\gamma & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    "0 \\le \\alpha_j^* \\le \\gamma & \\mbox{for } j = 1,\\ldots,N\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "where $\\alpha_j$ and $\\alpha_j^*$ is the dual for $\\xi_j$ and $\\xi_j^*$.  The weights are given by\n",
    "$$ \\beta = \\sum_j (\\alpha_j - \\alpha_j^*) h(X_{j\\cdot})\\,. $$\n",
    "\n",
    "**Question**:\n",
    "1. Can you write prediction function $f(x)$ in terms of the kernel $K$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Outlier Detection\n",
    "\n",
    "A classic application of SVM is Outlier Detection, which is actually an Unsupervised Learning technique.  The reason it is unsupervised is that one usually has data for \"normal\" times (**inliers**) but not data for \"abnormal\" times (**outliers**).  For example, you might be looking at server logs for either abnormally high activity that might indicate security breaches or a failure in your code.  One doesn't really understand what those failure modes are *a priori*.  What you do have is a lot of log data for when the server is behaving normally.\n",
    "\n",
    "There's a well-known modification to a two-class SVM with a Radial Kernel turning it into a *single-class* SVM by Sch√∂lkopf.  Mathematically, it is expressed as\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\xi, \\rho} \\frac{1}{2} \\|\\beta\\|^2 + \\frac{1}{\\nu n}\\sum_{j=1}^n \\xi_j - \\rho \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    " h(X_{j\\cdot}) \\cdot \\beta \\ge \\rho -\\xi_j & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\xi_j \\ge 0 & \\mbox{for } j = 1,\\ldots,N\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Notice that we are maximizing the value of $\\rho$ and that $\\rho$ forms a lower bound for $h(X_{j\\cdot}) \\cdot \\beta$ in our constraint.  Hence, we are pushing the points away from the origin in the transformed feature space (of course, there is the usual forgiveness in terms of $\\xi$'s).  Hence, instead of penalizing for being on the \"wrong\" side of the margin for a two-sided problem, we try to push the transformed features as far from the origin as possible.  In the original feature space, this creates a region near the training points which are considered \"regular\".  You can read more about this in the [Scikit Documentation](http://scikit-learn.org/stable/modules/outlier_detection.html).\n",
    "\n",
    "Observe that there is a parameter $\\nu$ that needs to be set.  This number sets both\n",
    "1. An upper bound on the fraction of training errors - training examples eroneously labeled as outliers.\n",
    "1. A lower bound of the fraction of support vectors - number of non-zero $\\xi$'s or $\\alpha$'s.\n",
    "\n",
    "$\\nu$ has to be set by the modeller and controls the number of false positives (training examples eroneously) and false negatives (although it is hard to know when a false negative occurs without training data).\n",
    "\n",
    "**Question** Why are these two notions equivalent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from sklearn import svm\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "# Generate train data\n",
    "X = 0.3 * np.random.randn(100, 2)\n",
    "X_train = np.r_[X + 2, X - 2]\n",
    "# Generate some regular novel observations\n",
    "X = 0.3 * np.random.randn(20, 2)\n",
    "X_test = np.r_[X + 2, X - 2]\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "# fit the model\n",
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"Novelty Detection\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='orange')\n",
    "\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([a.collections[0], b1, b2, c],\n",
    "           [\"learned frontier\", \"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.xlabel(\n",
    "    \"error train: %d/200 ; errors novel regular: %d/40 ; \"\n",
    "    \"errors novel abnormal: %d/40\"\n",
    "    % (n_error_train, n_error_test, n_error_outliers))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exit Tickets\n",
    "1. What information does an SVM model retain in memory after training?\n",
    "1. How do you decide which kernel to use?\n",
    "1. Explain to a layman what the advantages and drawbacks are of including a large margin in your optimization criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets, cross_validation, kernel_approximation, pipeline, preprocessing, random_projection\n",
    "    \n",
    "digits = datasets.fetch_mldata('mnist-original')\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X = X[y < 4]\n",
    "y = y[y < 4]\n",
    "\n",
    "N_SAMPLE=4000\n",
    "\n",
    "# shuffle the data, it'\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "X = np.array(X[indices[:N_SAMPLE], :], dtype=float)\n",
    "y = y[indices[:N_SAMPLE]]\n",
    "\n",
    "model = pipeline.Pipeline([\n",
    "    (\"Kernel Approximation\", kernel_approximation.Nystroem(kernel='poly', gamma=1e-1, degree=2)),\n",
    "    (\"Linear SVC\", svm.LinearSVC(C=1e1))\n",
    "])\n",
    "\n",
    "cv = cross_validation.KFold(len(y), n_folds=5)\n",
    "cross_validation.cross_val_score(model, X, y, scoring='accuracy', cv=cv).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (A trained model are simply the values $\\alpha$.  Here's a subtle trick: in a linear model, predicting the values of a linear SVM comes from $x \\cdot \\beta = \\sum_{\\alpha_j} y_j X_{j \\cdot} \\cdot x$.  For a nonlinear SVM, this can be generalized to $\\sum_{\\alpha_j} y_j K(X_{j \\cdot}, x)$\n",
    "1. (According to the Scikit documentation, this takes [between $O(pn^2)$ and $O(pn^3)$ time](http://scikit-learn.org/stable/modules/svm.html#complexity).  It's easy to see the latter (think about the cost of computing the objective function).\n",
    "1. Downsampling the more frequently occurring class and increase its `weight` accordingly\n",
    "1. For each of the kernels, $c$ determines a characteristic scale-length.  Look at the distribution of $x \\cdot x'$ or $\\|x - x'\\|^2$ in your data and choose $c$ roughly as the inverse standard deviation.\n",
    "1. All the ones that involve an inner product, not the one that involves the difference of features.  Note that $r$ can be aliased to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
