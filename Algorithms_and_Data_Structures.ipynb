{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some basic categories of \"coding interview\" or \"algorithm and data structure\" problems:\n",
    "\n",
    "1. Sorting\n",
    "2. Search\n",
    "3. Dynamic programming\n",
    "4. Graph theory\n",
    "\n",
    "We won't give any sort of complete treatment.  The goal is just to give one or two typical examples of each, and to sketch their solutions -- this should be a starting point from which you can go on to look at the (abundant) unworked problems out there.\n",
    "\n",
    "Further reading:\n",
    " - [From the wiki](https://sites.google.com/a/thedataincubator.com/the-data-incubator-wiki/employment/interview-techniques)\n",
    " - For a very large source of algorithm problems, see \n",
    " https://www.hackerrank.com/categories/algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "\n",
    "There are roughly three basic facts:\n",
    "\n",
    "1. It is a Theorem that any \"comparison sort\" (that is, a sort based only on comparing the things to be sorted for the predicate \"$a < b$\") can require no fewer than $O(n \\log n)$ steps for sorting an array of $n$ elements.  Some things worth saying:\n",
    "  - The $n$ here is the \"size of the input\" -- in this case, the number of elements in the array.\n",
    "  - What's this big-O notation?  We say that an algorithm \"is\" $O(n \\log n)$, in the size of the input $n$, if the amount of operations it performs is _bounded above_ by $C n \\log n$ for some constant $C$ independent of $n$ (but that can, in practice, be large).\n",
    "  - Best case / average case / worst case performance: We can ask that the bound above holds in the best / average / worst case amongst all possible inputs to the algorithm of size $n$. \n",
    "  - Note that this is just an upper bound, so that e.g. any algorithm that is $O(n \\log n)$ is also $O(n^2)$ since $n \\log n \\leq n^2$.  In practice, if we ask for _the_ \"big-O complexity of an algorithm\" we mean the best possible one.\n",
    "  \n",
    "2. The arguably most common comparison sorts are: bubble sort, merge sort, and quicksort.  \n",
    "  - __Bubble sort__ simply \"bubbles\" elements to their right location by swapping neighbors if they're out of order.\n",
    "  - __Merge sort__ splits the list in two, sorts each half, and then merges the results.  It has worst case complexity $O(n \\log n)$ -- the theoretical maximum.\n",
    "  - __Quick sort__ picks a \"pivot\" element and splits the list into the elements less than the pivot, the pivot, and the elements greater than the pivot -- then it does the same to each of the two halves above/below the pivot.  It has worst case complexity $O(n^2)$ but _average case_ complexity $O(n \\log n)$.\n",
    "   \n",
    "   In practice, quick sort runs faster -- this is heuristically because it has a lower constant $C$ (the real reasons are more complicated).\n",
    "  \n",
    "3. You can beat the dreaded $O(n \\log n)$ bound if you're not doing a comparison sort.  The simplest case is when you are sorting integers in a fixed small range: In that case, you can just go through and count the number of times each possible number occurred (\"counting sort\").  A more interesting variant is __radix (or bucket) sort__: You first do one pass to sort into buckets according to the first digit, then a pass in each bucket to sort into buckets according to the second digit, etc. for a total running time of $O(n d)$ where $d$ is the number of digits.\n",
    "  \n",
    "  \n",
    "Further reading:\n",
    "- For a convenient \"cheat sheet\" of big-O performance of various standard algorithms (not just sorting) see http://bigocheatsheet.com/.\n",
    "  \n",
    "### Exercises:\n",
    "1. What is bubble sort? (Hint: It \"bubbles\" elements to their right location by swapping neighbors if needed.)  What is its big-O complexity, and why?  Same for merge sort.\n",
    "2. When does quicksort take the maximal amount of time to run?  What are some strategies one might imagine for mitigating this?\n",
    "3. Suppose you needed to efficiently sort a trillion numbers.  How might you try to do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching\n",
    "\n",
    "Suppose you have an array of numbers (`haystack`) and you would like to look up whether a given number (`needle`) is in it, or to look up a key associated to it.  The obvious thing to do is to just loop over the list, comparing each entry.  If the list has $n$ entries, this will take time proportional to $n$.\n",
    "\n",
    "1. The most basic example of an alternate search strategy is __binary search__.  If the array `haystack` is known to be sorted, then we can do better: First check if needle is above or below the middle element of haystack, and then recurse and check again on only the right side.  This has running time $O(\\log n)$.  The one downside is the requirement to keep the list sorted -- this can be expensive if you want to be able to insert new elements into the middle.\n",
    "\n",
    "2. __Binary search trees__: To solve the problem in the previous sentence requires a new data structure.  A (binary) search tree is a binary tree (i.e. at most two children under every node) with values at each node, so that for each child the value of the left node is less than it, and the value of the right node is greater than it (with ties going in some systematic direction).  A _balanced_ binary search tree is one for which the height grows as $O(\\log n)$ as the number of nodes $n$ is increased by inserting new entries -- there are algorithms (key words: AVL tree, red-black tree, etc.) for maintaining balanced binary trees with $O(\\log n)$ insertion time.\n",
    "   In other words, a self-balancing binary search tree lets you make both the lookup (\"is needle in haystack\") and insert (\"put a new number into haystack\") take $O(\\log n)$ time.\n",
    "  \n",
    "3. __Priority queues__: A _priority queue_ is a container which can tell you the minimal element in $O(1)$ time, and to which you can add a new element (or remove the current minimal element) in $O(\\log n)$ time.  You can use balanced binary trees to implement this, and it is quite useful.\n",
    "\n",
    "Binary search, like quicksort and mergesort above, are examples of __divide and conquer__ algorithms: This is a common source of $\\log n$ factors!\n",
    "\n",
    "### Exercises:\n",
    "1. Give a precise description (say, as real Python code) of binary search.  Justify why it has running time $O(\\log n)$.\n",
    "2. Suppose that you want to compute $x^{N}$ for some large integer $N$.  What is the least number of multiplications you need in order to accomplish this?\n",
    "3. Challenge exercise (a semi-standard rather hard interview question): Suppose you have a list of numbers and you're planning on performing the following two operations over and over again: adding new elements and querying the median.  How quickly can this be done?  The buzz-word here is \"on-line median selection.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic programming\n",
    "\n",
    "Dynamic programming is roughly the conjunction of two things:\n",
    " - Recursion (over a bounded, partially ordered set of inputs); and,\n",
    " - Memoization, or remembering old values\n",
    " \n",
    "The simplest tautological example is computing Fibonacci numbers.  It is certainly recursive, given by \n",
    "\n",
    ">          \n",
    "    f(n) = f(n-1) + f(n-2)\n",
    "    \n",
    "But if you implement Fibonacci numbers this way on a computer, it will perform slowly.  This is because to compute $f(5)$ we need to first compute $f(4)$ and $f(3)$, but to compute $f(4)$ we need to compute $f(3)$ _again_.  It turns out that computing $f(n)$ in this naive fashion takes time proportional to $f(n)$ itself.  On the other hand, if we simply remember all Fibonacci numbers up to $f(n)$ -- so that we only compute each one once -- then it takes time proportional to $n$.\n",
    "\n",
    "(It is an exercise to observe that this is not the best possible thing to do: We can actually do it in time $O(\\log n)$ by re-framing it in terms of matrix multiplication, and using a divide-and-conquer trick for matrix multiplication.)\n",
    "\n",
    "Usually if you can reformulate your problem as a recursion over a bounded, partially ordered set of inputs you will be \"done\" just by sticking on a generic memoizer (e.g. good \"Python memoization decorator\").  In some cases, though, you can save some space by explicitly understanding what order you will want to compute the answer in and storing less (as seen in the Fibonacci example -- we only needed the last two values).\n",
    "\n",
    "### Exercises:\n",
    "1. Suppose given a string $s$.  Describe the best algorithm you can think of for finding the longest palindromic subsequence of $s$.\n",
    "2. Suppose given two strings $s$ and $t$.  Describe the best algorithm you can think of for finding the longest common subsequence of $s$ and $t$.\n",
    "3. Given an unsorted array, find the maximal length of a non-decreasing subsequence.\n",
    "4. Given an array of positive and negative numbers, find the sub-array having maximal sum.\n",
    "5. Do some problems from HackerRank, for instance https://www.hackerrank.com/challenges/grid-walking (in the graph section?!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph theory\n",
    "\n",
    "There are a few basic _fundamental_ things to be familiar with that we've already encountered previously:\n",
    "\n",
    "1. Search / traversal algorithms (+ shortest path algorithms): Breadth first search, depth first search, Dijkstra's algorithm.\n",
    "2. All-pairs shortest path: Floyd-Warshall.\n",
    "\n",
    "Using these, you can answer all sorts of related questions: For instance the search / traversal algorithms in 1 can be used to determine reachability or to break up a graph into connected components.\n",
    "\n",
    "If you look at something like HackerRank or other sources of algorithmic \"olympiad\" problems, a large chunk will be graph theory.  The above algorithms cover an awful lot of this -- of the remainder, the most common algorithms are ones for solving: \n",
    " - _topological sort_: Given a directed graph, you can think of it as a partial ordering of the vertices (i.e. A<B, A<D but B and D are not comparable).  Topological sort picks a total ordering compatible with the partial ordering (i.e. just declare B<D).\n",
    " - _minimal spanning tree_.\n",
    " - _max flow-min cut_.\n",
    " \n",
    " \n",
    "### Exercises:\n",
    "1. A graph has 12 edges and 6 nodes, each of which has degree 2 or 5. How many\n",
    "nodes are there of each degree?\n",
    "\n",
    "2. Let $K_n$ be the _complete graph_ on $n$-nodes -- that is, there is an edge between each pair of nodes.  What is the degree of each vertex of $K_n$.\n",
    "\n",
    "3. The idea of Floyd-Warshall is as follows:  Define a recursive function (with values in the positive reals and a placeholder $+\\infty$) by:\n",
    "$$ d(\\ell, s, t) = \\text{the shortest path from node $s$ to node $t$ that passes only through nodes $0, \\ldots, \\ell-1$ in the middle} $$\n",
    "so that for instance\n",
    "$$ d(0, s, t) = \\begin{cases}  0 & \\text{if $ s = t $} \\\\ \\text{length of the edge from $s$ to $t$} & \\text{if there is an edge from $s$ to $t$} \\\\ +\\infty & \\text{otherwise}  \\end{cases} $$\n",
    "is the base case.\n",
    " - Describe the inductive step\n",
    "$$ d(\\ell, s, t) = ... $$\n",
    " - Implement the algorithm in Python (or at least pseudo code)\n",
    " - How much memory does your implementation use?  Is this optimal?  What is its running time?\n",
    " \n",
    "4. Do some problems from HackRank, for instance https://www.hackerrank.com/challenges/journey-to-the-moon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets\n",
    "1. Describe x sort to a layman.\n",
    "1. Is x sort in-place? Is it stable?\n",
    "1. Describe some data well- and ill-suited to graph analysis.\n",
    "1. What kind of sort would you use for online updates to an already sorted list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
