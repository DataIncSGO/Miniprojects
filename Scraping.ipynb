{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll talk about \"scraping\": how to get unstructured data and turn it into something usable. We'll primarily focus on _web scraping_. Python has mature tools that make this pretty easy.\n",
    "\n",
    "The basic workflow is:\n",
    "\n",
    "1. Find the data you want on the web.\n",
    "2. Inspect the webpage and figure out how to select the content you want. This usually involves some combination of\n",
    "    - Viewing the source code of the page (especially if it is simple), and\n",
    "    - Figuring out the structure of the HTML parse tree.  This step is much easier with something like __Chrome Developer Tools__.\n",
    "3.  Write code to get out what you want:\n",
    "    - If the page is very simple, treat it as a bunch of text => __string manipulation / [regular expressions](https://docs.python.org/2/howto/regex.html)__ in Python.\n",
    "    - If the page is more complicated (and/or written in good style), we want to use the HTML parse tree => __[BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) / [lxml](http://lxml.de/lxmlhtml.html)__ in Python.\n",
    "4.  Make sure it worked!\n",
    "5.  If your crawling problem is at all non-trivial, you will now have to go back to Step 2 to zoom in further -- or you'll have parsed the URL of a link you want to follow, in which case you'll go back to Step 1 to figure out how to parse what you want from the new target page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, suppose we want to crawl the list of \"Available Technologies\" being licensed by MIT at http://technology.mit.edu and store their basic info, their associated patents, and the reference counts on their associated patents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding URLs\n",
    "\n",
    "Let's try to find the correct URL to use.\n",
    "\n",
    "- _First try_:  Aha, a list of links on the right.  Let's click on a few -- what do we see?  Many are empty, the categories are not obviously mutually exclusive, okay.  Maybe there's a better way.\n",
    "- _Second try_: Let's just search for all technologies at http://technology.mit.edu/technologies.  Okay, better but it only gives us 50 at a time.  We could just combine the four pages, that's fine.  Let's just click on page 2 to see what happens\n",
    "- _Third try_: Aha, the URL for page 2 is http://technology.mit.edu/technologies?limit=50&offset=50&query=.  That looks like we can just specify a higher limit and offset 0 and get the whole thing.\n",
    "- _Final answer_: Indeed, http://technology.mit.edu/technologies?limit=1000 has a giant list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://technology.mit.edu/technologies\"\n",
    "response = requests.get(url, params={\"limit\": 1000, \"offset\": 0})\n",
    "print response.url\n",
    "print response.text[:1000] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML and the DOM\n",
    "\n",
    "To get started:\n",
    "\n",
    "- Pull up http://technology.mit.edu/technologies?limit=1000 in Chrome.  \n",
    "- Open __View->Developer->Developer Tools__.  \n",
    "- Right click on one of the technology titles, and choose __\"Inspect Element\"__.\n",
    "\n",
    "What are we looking at?  Well... this is the structure of the webpage.  Nested _tags_ of different _types_ and having a variety of _attributes_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we learned above:\n",
    "\n",
    "  - All of the technologies are underneath (\"_descendents of_\")   `<div class=\"search\" id=\"nouvant-portfolio-content\">`\n",
    "  - In fact, each of them is in its own `<div class=\"technology\" data-images=\"true\" id=\"technology_XXXX\">`\n",
    "  \n",
    "Now we're ready to move on:\n",
    "\n",
    "## Parsing HTML\n",
    "Now, we need to parse the raw HTML and actually grab the links of detailed info. The two main parser libraries in Python are `BeautifulSoup` and `lxml`. `lxml` is much faster (it leverages several C libraries), but it's also worse at dealing with malformed, crummy HTML. Because parsing speed isn't our bottleneck here, we'll use `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.text)\n",
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parent_div = soup.find('div', attrs={'id': 'nouvant-portfolio-content'}) #Find (at most) *one*\n",
    "tech_divs = parent_div.find_all('div', attrs={'class':'technology'})  #Find *all*\n",
    "print len(tech_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS selectors\n",
    "\n",
    "This pattern of nested finds, based on tag type, ID, and class, is very common. It's so common that there are two special convenience languages for such traversals: [CSS selectors](http://www.w3schools.com/cssref/css_selectors.asp) and [XPath](http://www.w3schools.com/xsl/xpath_intro.asp) (which works for all XML, not just HTML). We'll be using CSS selectors, which are more common for HTML and easier to learn.\n",
    "\n",
    "With CSS selectors, we can write the above in a more concise and expressive way:\n",
    "\n",
    "```python\n",
    "tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "```\n",
    "\n",
    "All selectors work like 'find_all'.  Some basic building examples of selectors are:\n",
    "\n",
    " - `'mytag'` picks out all tags of type `mytag`.\n",
    " - `'#myid'` picks out all tags whose _id_ is equal to `myid`\n",
    " - `'.myclass'` picks out all tags whose _class_ is equal to `myclass`\n",
    " - `'mytag#myid'` will pick all tags of type `mytag` **and** `id` equal to `myid` (analogously for `'mytag.myclass'`)\n",
    " - If `'selector1'` and `'selector2'` are two selectors, then there is another selector `'selector1 selector2'`.  It picks out all tags satisfying `selector2` that are __descendents__(*) of something satisfying `selector1`, i.e. it's like our nested find.\n",
    " \n",
    " (*) It doesn't have to be a _direct_ descendent.  I.e. it can be a grand-grand-...-grand-child of something satisfying `selector1`.  For direct descendents we'd instead write `'selector1 > selector2'`\n",
    " \n",
    "Let's just see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print soup.select('div#nouvant-portfolio-content')[0].prettify()[:400]\n",
    "print\n",
    "print soup.select('div#nouvant-portfolio-content div.technology')[0].prettify()[:400]\n",
    "tech_divs = soup.select('div#nouvant-portfolio-content div.technology')\n",
    "print\n",
    "print len(tech_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to pull out some key pieces of info:\n",
    "\n",
    "- The technology's \"title\" (the text in the `<a>` element)\n",
    "- The link to follow for more info on the technology (the _href_ attribute of the `<a>`)\n",
    "- And a short blurb about the text (in the `<span>`)\n",
    "\n",
    "Let's write some code to extract this.  But before we do, let's discuss what _form_ the output should take: It is often convenient to store data in a dictionary (i.e. as a _key-value_ hashtable) - in other words, to name the bits of data you are collecting.  One big advantage is that this makes it easier to add in extra fields progressively.\n",
    "\n",
    "Let's see what the code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "firsta = tech_divs[0].select('a')[0]\n",
    "print firsta.text\n",
    "print firsta['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "# We're going to use a \"named tuple\" to store our key-value data.\n",
    "# We could also have used a dictionary, with strings as keys.\n",
    "# Named tuples have some advantages:\n",
    "#  - Better notation with autocomplete, x.field_name instead of x['field_name']\n",
    "#  - If you change your object structure later and fail to update your\n",
    "#    code to include the new fields, this will make it easier to find.\n",
    "#  - They are immutable, preventing certain sorts of bugs.\n",
    "# ... and some disadvantages:\n",
    "#  - If you want to augment object structure you need a new type\n",
    "#    (or to go back and fill your code)\n",
    "#  - They are immutable.\n",
    "##\n",
    "from collections import namedtuple\n",
    "TechBasic = namedtuple('TechBasic', 'title, url, short')\n",
    "\n",
    "def td_info(td):\n",
    "    la = td.select('h2 > a')\n",
    "    ls = td.select('span')\n",
    "    if len(la) != 1 or len(ls) != 1:\n",
    "        print \"Uh oh! We did something wrong for:\"\n",
    "        print \"\\n\".join(\">>> \" + line for line in td.prettify().split(\"\\n\"))\n",
    "        return\n",
    "    return TechBasic(title=la[0].text, url=la[0]['href'], short=ls[0].text)\n",
    "\n",
    "tech_links = [td_info(td) for td in tech_divs if td_info(td) is not None]\n",
    "\n",
    "print tech_links[0].title\n",
    "print\n",
    "print tech_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching subsequent pages\n",
    "\n",
    "We'll often want to scrape subsequent pages for more detailed data.  If there are many such pages, this can be slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urlparse import urljoin\n",
    "\n",
    "Patent = namedtuple('Patent', 'name url')\n",
    "TechDetailed = namedtuple('TechDetailed', 'tech_basic, patents')\n",
    "\n",
    "url_base=\"http://technology.mit.edu/\"\n",
    "\n",
    "def get_tech_details(response, tech_basic):\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    patents = [Patent(name=a.text, url=a[\"href\"])\n",
    "               for a in soup.select('dd.us_patent_issued a')]\n",
    "    return TechDetailed(tech_basic=response.url, patents=patents)\n",
    "\n",
    "tech_details = [get_tech_details(requests.get(urljoin(url_base, tech_basic.url)),\n",
    "                                 tech_basic)\n",
    "               for tech_basic in tech_links[:2]]\n",
    "print tech_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "In the last code segment, we only did the first one.  If we try to get them all this way, it'll take a while.  Run the next cell for as long (or not) as you wish, and when you get bored use _Kernel->Interrupt_ to stop it.\n",
    "\n",
    "The problem is that connecting to a remote server and fetching the pages takes a while. Scraping web pages is usually _IO-bound_ and not CPU-bound (that is, we spent most of our time waiting for data and not processing it). Fortunately, Python gives us lots of different ways to deal with this problem.\n",
    "\n",
    "We'll be using [requests-futures](https://github.com/ross/requests-futures), a nice wrapper combining `requests` with `concurrent.futures`. For a faster, though harder to debug, alternative, you can look at [grequests](https://github.com/kennethreitz/grequests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LIMIT = 10\n",
    "urls = [urljoin(url_base, tech_basic.url) for tech_basic in tech_links]\n",
    "urls[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 1:** The first solution is to run the requests serially.  This is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n1\n",
    "# Slow version\n",
    "\n",
    "tech_details = [get_tech_details(requests.get(url), tech_basic)\n",
    "                for url, tech_basic in zip(urls, tech_links)[:LIMIT]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Solution 2:** We can use Python's [multiprocessing](https://docs.python.org/2/library/multiprocessing.html) interface, which can easily parallelize a map.  This is a very straightforward API to use.  The drawback of this is that it spins up independent processes, which have a potentially significant download time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n1\n",
    "# faster version -- using multiprocessing\n",
    "\n",
    "from multiprocessing import Pool\n",
    "p = Pool(3)\n",
    "responses = p.map(requests.get, urls[:LIMIT])\n",
    "tech_details = [get_tech_details(response, tech_basic)\n",
    "                for response, tech_basic in zip(responses, tech_links[:LIMIT])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 3:** For reqeusts, there is a special library called `requests_futures` which returns a placeholder object that holds a promise to return the webpage sometime later (in the \"future\").  This allows us to continue making other fetching requests while waiting for the first result to return.\n",
    "\n",
    "![Synchronous vs. Asynchronous](images/async.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n1\n",
    "# faster version using requests-futures\n",
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "session = FuturesSession(max_workers=5)\n",
    "futures = [session.get(url) for url in urls[:LIMIT]]\n",
    "\n",
    "tech_details = [get_tech_details(future.result(), tech_basic)\n",
    "                for future, tech_basic in zip(futures, tech_links)[:LIMIT]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Let's put all of that together.  Write a function \n",
    "```python\n",
    "def get_tech_basics(url):\n",
    "    ...\n",
    "```\n",
    "\n",
    "that returns `TechBasic` for each technology on the page.  Combine this with the pooled requests to get_tech_details to obtain a list of TechDetails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin.**\n",
    "That's it, we now have a basic not-entirely-trivial example. We took some detours along the way, so let's just take a look at what our code looks like without those detours:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "\n",
    "1. Modify \"get_tech_details\" to get other interesting information on the technology, like a long form description and/or the authors' names.  (You'll also want to modify TechDetailed.  Do that first and note that now the code breaks when it tries to construct a TechDetailed with the wrong number of fields.)\n",
    "\n",
    "2. Modify \"get_tech_details\" to try to follow the link and to get more information on the patent -- for instance when it was filed and granted, or how many other patents reference it.  (Warning: The patent web site is much less regular than MIT's!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy in Python\n",
    "\n",
    "If you are really interested in crawling, consider using `scrapy`.  [Scrapy](http://scrapy.org/) is a specialized python package for scraping websites.  In particular, it has a few features:\n",
    "1. The HTML is parsed and accessed through a `response` object in a `parse` method which in turn supports `response.xpath` and `response.css` methods, allowing one to use `xpath` and `css` selectors on the response dom, respectively.\n",
    "1. Data is stored in `scrapy.Item` objects (which are similar to `namedtuple`s) or as python dictionaries.\n",
    "1. Scrapy is object-oriented and calls it's own `parse` method (a generator) that `yield`s values.\n",
    "1. You can limit your crawls through specifying the class property `allowed_domains` and definte the starting point of your crawl using the class property `start_urls`.\n",
    "1. You can also build pipelines of crawling and data extraction steps to make sure crawling code more usable.\n",
    "1. Additional scraping steps (e.g. scraping entries in a directory like in the example above) can be accessed via `scrapy.Request`.\n",
    "1. It has command lines arguements to allow you to interactively play with the the `response` object from a webpage (`scrapy shell`) or view a page as the library renders it, which may be different from how your browser renders it (`scrapy view`).\n",
    "\n",
    "The following is a canonical `scrapy` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class StackOverflowSpider(scrapy.Spider):\n",
    "    name = 'stackoverflow'\n",
    "    start_urls = ['http://stackoverflow.com/questions?sort=votes']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for href in response.css('.question-summary h3 a::attr(href)'):\n",
    "            full_url = response.urljoin(href.extract())\n",
    "            yield scrapy.Request(full_url, callback=self.parse_question)\n",
    "\n",
    "    def parse_question(self, response):\n",
    "        yield {\n",
    "            'title': response.css('h1 a::text').extract()[0],\n",
    "            'votes': response.css('.question .vote-count-post::text').extract()[0],\n",
    "            'body': response.css('.question .post-text').extract()[0],\n",
    "            'tags': response.css('.question .post-tag::text').extract(),\n",
    "            'link': response.url,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complicated example\n",
    "Suppose we had picked Stanford instead of MIT.  Let's try to do the same thing (it's a bit harder to get a good listing URL, so I just downloaded one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from urlparse import urljoin\n",
    "\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import requests\n",
    "\n",
    "with open(\"small_data/Stanford-Tech-Listing.html\", \"r\") as fin:\n",
    "    soup = BeautifulSoup(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#BeautifulSoup doesn't seem to support 'or' selectors, so:\n",
    "selector = lambda x: x.has_attr(\"id\") and x[\"id\"].startswith(\"output_row\")\n",
    "tech_rows = soup.find_all(selector)[1:]\n",
    "# Alternate -- showing how to go up and down the tree\n",
    "#tech_rows = soup.find('tr', attrs={'id':'output_row_1'}).parent.findAll('tr')[1:]\n",
    "print len(tech_rows)\n",
    "print tech_rows[0].prettify()\n",
    "print tech_rows[-1].prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details:** Let's quickly break down that last line for two bits of Python syntax that we haven't explicitly talked about\n",
    "    >    selector = lambda x: x.has_attr('id') and x['id'].startswith('output_row')\n",
    "This is a \"lambda expresion\" -- a short, inline, unnamed function. Lambdas are pretty limited, so you should define a named function for anything complicated  \n",
    "\n",
    "    >    tech_rows = soup.find_all(selector)[1:]      \n",
    "                                            ^^^^\n",
    "This is list slice notation (we already used this above with [:2]!).  In this case, we're taking all but the zero-th entry (which is a list header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UH OH:** \n",
    "When originally preparing this, I was using Anaconda.  The same code only showed about _254_ of the _1727_ entries -- BeautifulSoup was incorrectly parsing the file.  These sorts of things are not entirely uncommon, so sometimes it helps to double-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warning: This is hacky code!\n",
    "TechBlurb = namedtuple('TechBlurb', 'docket techid url title')\n",
    "def parse_tr(tr):\n",
    "    link = tr.select(\"td.output_data a\")[0]\n",
    "    \n",
    "    docket = link.text\n",
    "    url = link[\"href\"]\n",
    "    techid = url.split(\"=\")[1]\n",
    "    title = tr.select(\"td.output_data\")[2].text\n",
    "    return TechBlurb(docket=docket, techid=techid, url=url, title=title)\n",
    "tech_blurbs=map(parse_tr, tech_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And this isn't much better!\n",
    "def find_comment_by_text_in(soup, comment_text):\n",
    "    return soup.find(text=lambda text: isinstance(text, Comment) and comment_text in text)\n",
    "\n",
    "TechDetailed = namedtuple('TechDetailed', 'blurb, abstract, similar')\n",
    "SimilarHint = namedtuple('SimilarHint', 'techid, docket, title')\n",
    "def get_tech_details(response):\n",
    "    # We're doing a lot of chaining with implicit assumptions here -- \n",
    "    #   it might fail in all sorts of way, in which case we give up.\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    contents = soup.find_all('form')[1]\n",
    "    abstract = (find_comment_by_text_in(contents, 'Abstract')\n",
    "        .find_next_sibling('hr')\n",
    "        .find('div')\n",
    "        .text)\n",
    "        \n",
    "    def parse_similar_tr(r):\n",
    "        tds = r.find_all('td')\n",
    "        if len(tds) < 3:\n",
    "            return None\n",
    "        return SimilarHint (\n",
    "            techid = tds[0].find('a')['href'].split('=')[1], \n",
    "            docket = \"S\"+tds[0].text.strip(), \n",
    "            title  = tds[2].text.strip()\n",
    "        )\n",
    "\n",
    "    similar_trs = (find_comment_by_text_in(soup.find_all('form')[1], 'Similar Tech')\n",
    "                      .find_next_sibling('table')\n",
    "                      .find('div')\n",
    "                      .find('table')\n",
    "                      .find('table')\n",
    "                      .find_all('tr'))\n",
    "    similar = filter(None, [parse_similar_tr(tr) for tr in similar_trs])\n",
    "    \n",
    "    return TechDetailed(blurb=blurb, abstract=abstract, similar=similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Since the point is to show that something goes wrong, let's not wait until the end!\n",
    "# imap_unordered lets you use the results of the map as they are produced (rather than storing them)\n",
    "# and with no guarantee on order.\n",
    "\n",
    "url_base=\"http://techfinder.stanford.edu/\"\n",
    "\n",
    "for blurb in tech_blurbs:\n",
    "    response = requests.get(urljoin(url_base, blurb.url))\n",
    "    try:\n",
    "        get_tech_details(response)\n",
    "    except:\n",
    "        print \"Something went wrong!\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Remark:\n",
    "When we run the above code, it tells us that [this technology](http://techfinder.stanford.edu/technology_detail.php?ID=30261) did not have a list of similar technologies.  But going to the web page shows that it does!  What went wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://techfinder.stanford.edu/technology_detail.php'\n",
    "soup = BeautifulSoup(requests.get(url, params={\"ID\": 30261}).text)\n",
    "contents = soup.find_all('form')[1]\n",
    "print contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go and look at the same part of the **raw** HTML, we find that there is no `</form>` there:\n",
    "\n",
    "    >    <!--- Applications --->\n",
    "    >    <h3>Applications</h3><br/>\n",
    "    >    <ul><li>Imaging apoptosis<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Research</li><li>Clinical<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Monitor therapeutic efficacy in cancer patients</li><li>Anti-cancer drug selection</ul></ul></li></ul><br/>\n",
    "    >    \n",
    "    >    <!--- Advantages --->\n",
    "    >    <h3>Advantages</h3><br/>\n",
    "    >    <ul><li>High specificity for caspase-3 and -7</li><li>High sensitivity</li><li>Non-invasive</li><li>Biocompatible</li><li>Small size of probe allows:<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Deep tissue penetration</li><li>More extensive biodistribution</ul></li><li>PET probes:<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>High tumor/muscle ratio in apoptotic tumors</li><li>High uptake value in apoptotic tumors</ul></li><li>Fluorescent probe:<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Possess NIR spectral properties</ul></li><li>May help promote personalized cancer medicine</li><li>Potential for probe design strategy to be applied to other enzyme targets</li></ul><br/>\n",
    "\n",
    "What there **is** is _mal-formed HTML_ that is bad enough to confuse BeautifulSoup.  (Note that it's not nearly bad enough to confuse a web browser however).  If you look at more examples, you will find even worse ones -- a stray `</html>` in the middle of a document is not unheard of.  \n",
    "\n",
    "To fix this, we can pre-\"tidy\" the page before feeding it to BeautifulSoup using **pytidylib**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tidylib import tidy_document\n",
    "url='http://techfinder.stanford.edu/technology_detail.php'\n",
    "\n",
    "tidy_page, __ = tidy_document(requests.get(url, params={\"ID\": 30261}).text)\n",
    "soup = BeautifulSoup(tidy_page)\n",
    "contents = soup.find_all('form')[1]\n",
    "print contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Go back and modify `get_tech_details` to use this 'tidy' approach.\n",
    "\n",
    "2. Sometimes web servers are slow and/or unreliable, and sometimes your connection is.  If we were to run the above test twice, we'd probably find that some of the failures were just due to a connection error.  We didn't notice this because the _outer_ `try` / `except` is also catching these.  So: Modify `get_tech_details` to allow up to 3 retries. <br/>Bonus points if you actually look at what exceptions `urllib` throws in those cases instead of a general catch-all mechanism.  Alternate type of bonus points if you figure out how to do it using the `retrying` package.  You can test these by throttling your internet on and off to simulate an unreliable connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets\n",
    "1. Write a regex to parse numerical furniture prices from a string of descriptive text which contains other numbers.\n",
    "1. How would you design a web scraping app such that the user interface remained responsive? One that is robust to poor internet connections?\n",
    "1. How would you deal with messy/malformatted HTML/XML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_futures.sessions import FuturesSession\n",
    "import requests\n",
    "\n",
    "# Getting the list of short 'blurbs' about the techs\n",
    "TechBasic = namedtuple('TechBasic', 'title, url, short')\n",
    "def get_tech_basics(url):\n",
    "    url = \"http://technology.mit.edu/technologies?limit=1000\"\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "\n",
    "    ## Get the list of tech blurbs\n",
    "    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "\n",
    "    ## Parse a single 'td' on the index page\n",
    "    def td_info(td):\n",
    "        la = td.select('h2 > a')\n",
    "        ls = td.select('span')\n",
    "        if len(la) != 1 or len(ls) != 1:\n",
    "            print \"Uh oh! We did something wrong\"\n",
    "            return\n",
    "        return TechBasic(title=la[0].text, url=la[0]['href'], short=ls[0].text)\n",
    "    \n",
    "    return [td_info(td) for td in tech_divs]\n",
    "\n",
    "\n",
    "# Adding in some details (just patent info, for now)\n",
    "Patent = namedtuple('Patent', 'name url')\n",
    "TechDetailed = namedtuple('TechDetailed', 'tech_basic, patents')\n",
    "\n",
    "def get_tech_details(response):\n",
    "    soup = BeautifulSoup(response.text)\n",
    "\n",
    "    def patent_info(a):\n",
    "        return Patent(name=a.text, url=a['href'])\n",
    "    patents = [patent_info(a) for a in soup.select('dd.us_patent_issued a')]\n",
    "\n",
    "    return TechDetailed(tech_basic=tech_basic, patents=patents)\n",
    "\n",
    "## The main driver code:\n",
    "tech_basics = get_tech_basics(\"http://technology.mit.edu/technologies?limit=1000\")\n",
    "url_base=\"http://technology.mit.edu/\"\n",
    "session = FuturesSession(max_workers=15)\n",
    "futures = [session.get(url_base + tech_basic.url)\n",
    "       for tech_basic in tech_basics if tech_basic is not None]\n",
    "tech_details = [get_tech_details(future.result()) for future in futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(tech_details)\n",
    "print tech_details[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
