{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to be able to clearly understand and explain the theoretical and practical differences between machine learning algorithms. This notebook (for now) is a collection of resources to help you understand the landscape of algorithms as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors to consider\n",
    "\n",
    "A few factors to consider:\n",
    "1. What are you trying to predict?  Should the algorithm be a classification, regression, or clustering problem?\n",
    "1. How does the algorithm scale to larger datasets in terms of both **memory** or **time**.  Does this make the computation infeasible?\n",
    "1. Is there an **iterative** (versus **batch**) version of this algorithm?  Is **online learning** or **streaming** possible?\n",
    "1. Do you have to worry about explicability?\n",
    "1. Are there many features?  If there are, do you want to reduce the dimension (e.g. PCA or Lasso)?\n",
    "1. **Accuracy**: does the algorithm tend to underfit, (e.g. it doesn't satisfy the asymptotic approximation property)  A fixed model form is less able to take advantage of more data than a flexible model form.\n",
    "1. Does the \"prior\" you're introducing with a **parametric** model make sense for your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowchart to summarize the above\n",
    "\n",
    "This flowchart ([interactive source](http://scikit-learn.org/stable/tutorial/machine_learning_map/)) has some good examples of the kinds of criteria one needs to be thinking about when choosing an algorithm.\n",
    "![Machine learning flowchart from the scikit-learn documentation](images/ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods\n",
    "For decision trees, remember that progressing from decision trees -> random forests -> gradient-boosting trees\n",
    "1. increases accuracy\n",
    "1. decreases explicability\n",
    "1. increases computation time/memory footprint (boosting algorithms can't be parallelized)\n",
    "\n",
    "### Bagging\n",
    "Random forests demonstrate the idea of bootstrap aggregation, or bagging. Each individual model has an equal vote in the ensemble, and you have the freedom to skew the weak learners towards higher variance, knowing that the averaging will (ideally) wash out the randomness and prevent overfitting.\n",
    "\n",
    "### Boosting\n",
    "Gradient boosting trees demonstrate the power of training on the residual from your own predictions in order to achieve very good prediction metrics. Other boosting algorithms offer similar benefits, as well as similar drawbacks. Beware of overfitting here.\n",
    "\n",
    "### Blending\n",
    "The FeatureUnion full_model you implemented in ml.py is an example of combining the predictions of many (usually simple) models and passing them as features to a final regressor or classifier. Using a linear model is equivalent to taking a weighted average of the contributors, whereas a more complex final estimator is capable of combining the component models nonlinearly. In general, the explicability of these techniques is slightly better than a \"black box\" algorithm like a neural network.\n",
    "\n",
    "You might notice that in eg. Kaggle competitions, many of the winning entries are based on ensemble models. Ensembles tend to outperform individual models, but require careful tuning of each of the components, as well as more computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image ([source code](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)) shows how various scikit-learn classifers fit their models for a few different configurations of training data.\n",
    "![Comparison of various scikit-learn classifiers from the official documentation](images/plot_classifier_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A small note about model output\n",
    "You might think after learning about complex classification models: \"why would anyone ever use plain old logistic regression?\" It's important to remember that the probabilistic output of logistic regression is valuable in certain circumstances. In general, consider that beyond just comparing error metrics with other algorithms, you may sometimes want to compare output with entirely different methods of analysis - for example, if you're working with an actuary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
