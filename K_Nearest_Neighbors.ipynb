{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors\n",
    "*&copy; The Data Incubator*\n",
    "\n",
    "In this notebook we'll explore the K Nearest Neighbors algorithm.  As the name implies, the prediction for a new point is based on the closest $k$ points in the training set (where closest is usually defined in terms of Euclidiean distance on the $p$-dimensional feature space).  In math terms, we're taking the Euclidean distance between the $p$-vectors $X_{j\\cdot}$ as our metric.  As a classificaiton algorithm, it takes a majority vote of the nearest $k$ neighbors.  As a regression algorithm, it takes the average label of the nearest $k$ neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load and display a sample of the iris dataset\n",
    "\n",
    "from sklearn import neighbors, datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the data\n",
    "# The iris data set labels 3 flower types (y) by 4 different attributes of the flower (X).\n",
    "# For more about the attributes, see http://mldata.org/repository/data/viewslug/datasets-uci-iris/\n",
    "data = datasets.load_iris()\n",
    "X = pd.DataFrame(data.data, columns=[\"sepal_length\", \"sepal_width\", \"petal_legnth\", \"petal_width\"])\n",
    "y = pd.Series(data.target)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process the nearest neighbors\n",
    "nearest_neighbors = neighbors.NearestNeighbors(n_neighbors=5, algorithm='ball_tree')\n",
    "nearest_neighbors.fit(X)\n",
    "distances, indices = nearest_neighbors.kneighbors(X)\n",
    "\n",
    "# Column $k$ gives the distances between each point and its $k$-th nearest neighbor from 0 to 4\n",
    "# Column 0 is always itself.\n",
    "# Each row is a separate point (note the first column)\n",
    "print \"Distance\"\n",
    "print pd.DataFrame(distances).head()\n",
    "print \"Indices\"\n",
    "print pd.DataFrame(indices).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We illustrate the effect of the number of neigbhors used on accruacy.\n",
    "\n",
    "from sklearn import neighbors, cross_validation, grid_search\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(len(y), n_iter=20, test_size=0.2, random_state=42)\n",
    "param_grid = { \"n_neighbors\": range(4, 24, 4) }\n",
    "nearest_neighbors_cv = grid_search.GridSearchCV(neighbors.KNeighborsClassifier(), \n",
    "                                                param_grid=param_grid, cv=cv, \n",
    "                                                scoring='accuracy')\n",
    "\n",
    "nearest_neighbors_cv.fit(X, y)\n",
    "cv_accuracy = pd.DataFrame.from_records(\n",
    "    [(score.parameters['n_neighbors'],\n",
    "      score.mean_validation_score)\n",
    "     for score in nearest_neighbors_cv.grid_scores_],\n",
    "columns=['n_neighbors', 'accuracy'])\n",
    "\n",
    "plt.plot(cv_accuracy.n_neighbors, cv_accuracy.accuracy)\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "1. Is this algorithm parametric on non-parametric?\n",
    "1. As `n_neighbors` gets larger, the model becomes more accurate and then less.  What's happening?\n",
    "1. For the classification version, what happens when you have one class being over represented?\n",
    "1. The notion of distance in nearest neighbors is Euclidan in feature space.  For what kinds of input data might this be a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Data.\n",
    "\n",
    "Based on your answer to the last question, we will want to scale the individual features so that they have similar variance.  For each feature, we are going to subtract the mean and divide by the standard deviation:\n",
    "\n",
    "$$ X_{ji}' = \\frac{X_{ji} - \\mu_i}{\\sigma_i} $$\n",
    "\n",
    "where $\\mu_i$ is the mean of the $i$-th column (or feature) and $\\sigma_i$ is the standard deviation.\n",
    "\n",
    "**Question**: For $k$-Nearest-Neighbors, was subtracting the mean necessary?  For what other algorithms might subtracting the mean help?\n",
    "\n",
    "**Exercise**: Let's put all each feature of the dataset on the same scale by normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# first, let's see what the standard deviations are.\n",
    "scaler = preprocessing.StandardScaler(copy=True).fit(X)\n",
    "pd.DataFrame({\n",
    "    \"Mean\": scaler.mean_,\n",
    "    \"Std\": scaler.std_\n",
    "}, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: It looks like some of the features are on a larger numerical scale.  Let's put all the data on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_copy = X.copy()  # Feature (bug?): transform modifies the original \n",
    "X_scaled = scaler.transform(X_copy)\n",
    "pd.DataFrame({\n",
    "    \"Mean\": X_scaled.mean(axis=0), \n",
    "    \"Std\": X_scaled.std(axis=0)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are going to first scale and then apply KNeighbors.\n",
    "# Pipeline allows us to chain together multiple transformers and then a regressor or classifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scaled_nearest_neighbors = Pipeline([('scaling', preprocessing.StandardScaler(copy=True)), \n",
    "                                     ('neighbors', neighbors.KNeighborsClassifier())])\n",
    "\n",
    "param_grid = { \"neighbors__n_neighbors\": range(4, 24, 4) }    # parameters to Pipeline take the form [label]__[estimator_param]\n",
    "scaled_nearest_neighbors_cv = grid_search.GridSearchCV(scaled_nearest_neighbors, \n",
    "                                                       param_grid=param_grid, cv=cv, \n",
    "                                                       scoring='accuracy')\n",
    "\n",
    "scaled_nearest_neighbors_cv.fit(X, y)\n",
    "scaled_cv_accuracy = pd.DataFrame.from_records(\n",
    "    [(score.parameters['neighbors__n_neighbors'],\n",
    "      score.mean_validation_score)\n",
    "     for score in scaled_nearest_neighbors_cv.grid_scores_],\n",
    "columns=['n_neighbors', 'accuracy'])\n",
    "\n",
    "plt.plot(scaled_cv_accuracy.n_neighbors, scaled_cv_accuracy.accuracy, label=\"scaled_cv_accuracy\")\n",
    "plt.plot(cv_accuracy.n_neighbors, cv_accuracy.accuracy, label=\"cv_accuracy\")\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='lower center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "1. [Wikipedia](http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "1. The Scikit Learn package implements [K Nearest Neighbors](http://scikit-learn.org/stable/modules/neighbors.html).  It has both a [`KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and [`KNeighborsRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exit Tickets\n",
    "1. In KNN, does most computation take place during training or during testing?\n",
    "1. After training, what must the model retain in memory in order to make predictions?\n",
    "1. What are algorithms like KD-Ball and KD-Tree meant to do? And what's their drawback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers\n",
    "\n",
    "1. The algorithm is non-parametric.\n",
    "1. As `n_neighbors` gets larger, we're increasing variance and decreasing bias so we expect to see greater and than less accuracy.\n",
    "1. For the classification version, if one class is over-represented, then it will always win a majority vote.  Using an average of the nearest $k$ neighbors as a predictor may be useulf.\n",
    "1. The notion of distance in nearest neighbors is Euclidan in feature space.  This is really bad if the features have very different scales.\n",
    "\n",
    "1. It is probably good to normalize the data by dividing all the features by their standard deviation.  Note that subtracting a mean is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
