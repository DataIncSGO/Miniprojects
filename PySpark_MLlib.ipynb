{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib\n",
    "*Official documentation [here](https://spark.apache.org/docs/latest/mllib-guide.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "print sc.version  # should be >= 1.5.1 for distributed matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed to convert RDDs into DataFrames\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Data Types\n",
    "### Vector\n",
    "A mathematical vector. MLlib supports both dense vectors, where every entry is stored, and sparse vectors, where only the nonzero entries are stored to save space. Vectors can be constructed with the mllib.linalg.Vectors package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,[0,2],[1.0,3.0])\n",
      "(3,[0,2],[1.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors, SparseVector\n",
    "\n",
    "# Create a dense vector (1.0, 0.0, 3.0) from a NumPy array.\n",
    "dv1 = np.array([1.0, 0.0, 3.0])\n",
    "\n",
    "# Create a dense vector (1.0, 0.0, 3.0) from a Python list.\n",
    "dv2 = [1.0, 0.0, 3.0]\n",
    "\n",
    "# Create a SparseVector (1.0, 0.0, 3.0) by specifying its indices and values corresponding to nonzero entries.\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
    "\n",
    "# Create a sparse vector (1.0, 0.0, 3.0) by specifying its nonzero entries.\n",
    "sv2 = Vectors.sparse(3, [(0, 1.0), (2, 3.0)])\n",
    "\n",
    "print sv1\n",
    "print sv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabeledPoint\n",
    "A labeled data point for supervised learning algorithms such as classification and regression. Includes a feature vector and a label (which is a floating-point value). Located in the mllib.regression package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 -1.0\n",
      "========\n",
      "[1.0,0.0,3.0] (3,[0,2],[1.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "point1 = LabeledPoint(1.0, np.array([1.0, 0.0, 3.0]))\n",
    "\n",
    "point2 = LabeledPoint(-1.0, SparseVector(3, [0, 2], [1.0, 3.0]))\n",
    "\n",
    "print point1.label, point2.label\n",
    "print \"========\"\n",
    "print point1.features, point2.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local matrix\n",
    "*Integer*-typed row and column indices and double-typed values, stored on a single machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed matrix \n",
    "*Long*-typed row and column indices and double-typed values, stored distributively in one or more RDDs.  They come in three formats:\n",
    "\n",
    "- **RowMatrix:** Row-oriented distributed matrix without meaningful row indices, e.g. a collection of feature vectors. It is backed by an RDD of its rows, where each row is a local vector. We assume that the number of columns is not huge for a RowMatrix so that a single local vector can be reasonably communicated to the driver and can also be stored / operated on using a single node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 3\n",
      "cols: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# Create an RDD of vectors.\n",
    "rows = sc.parallelize([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n",
    "\n",
    "# Create a RowMatrix from an RDD of vectors.\n",
    "rowMatrix = RowMatrix(rows)\n",
    "\n",
    "# return size\n",
    "m = rowMatrix.numRows()\n",
    "n = rowMatrix.numCols() \n",
    "\n",
    "print \"rows: %d\\ncols: %d\" % (m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **IndexedRowMatrix:** Similar to a RowMatrix but with meaningful row indices. It is backed by an RDD of indexed rows, so that each row is represented by its index (long-typed) and a local vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 4\n",
      "cols: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "\n",
    "indexedRows = sc.parallelize([IndexedRow(0, [1.0, 2.0]),\n",
    "                              IndexedRow(1, [2.0, 3.0]), \n",
    "                              IndexedRow(3, [4.0, 5.0])])\n",
    "\n",
    "indexedRowMatrix = IndexedRowMatrix(indexedRows)\n",
    "\n",
    "# return size\n",
    "m = indexedRowMatrix.numRows()\n",
    "n = indexedRowMatrix.numCols() \n",
    "\n",
    "print \"rows: %d\\ncols: %d\" % (m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **CoordinateMatrix:** (i.e. a distributed sparse matrix) is (essentially) a list of `(Long, Long, Double)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 3\n",
      "cols: 3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "\n",
    "matrixEntries = sc.parallelize([MatrixEntry(0, 0, 1.),\n",
    "                                MatrixEntry(1, 1, 1.),\n",
    "                                MatrixEntry(2, 2, 1.)])\n",
    "\n",
    "coordinateMatrix = CoordinateMatrix(matrixEntries)\n",
    "\n",
    "# return size\n",
    "m = coordinateMatrix.numRows()\n",
    "n = coordinateMatrix.numCols() \n",
    "\n",
    "print \"rows: %d\\ncols: %d\" % (m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BlockMatrix:** A distributed matrix backed by an RDD of MatrixBlocks, where a MatrixBlock is a tuple of ((Int, Int), Matrix), where the (Int, Int) is the index of the block, and Matrix is the sub-matrix at the given index with size rowsPerBlock x colsPerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "\n",
    "BlockMatrix = coordinateMatrix.toBlockMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, because the matrix is stored in a distributed way, converting between matrix formats is expensive!\n",
    "\n",
    "**Rating**  \n",
    "A rating of a product by a user, used in the `mllib.recommendation` package for product recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine-learning in MLlib\n",
    "\n",
    "Spark supports a number of machine-learning algorithms.\n",
    "\n",
    "- Classification and Regression\n",
    "    - SVM, linear regression\n",
    "    - SVR, logistic regression\n",
    "    - Naive Bayes\n",
    "    - Decision Trees\n",
    "    - Random Forests and Gradient-Boosted Trees\n",
    "- Clustering\n",
    "    - K-means (and streaming K-means)\n",
    "    - Gaussian Mixture Models\n",
    "    - Latent Dirichlet Allocation\n",
    "- Dimensionality Reduction\n",
    "    - SVD and PCA\n",
    "- It also has support for lower-level optimization primitives:\n",
    "    - Stochastic Gradient Descent\n",
    "    - Low-memory BFGS and L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.302090044981\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD, LinearRegressionModel\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "import random\n",
    "\n",
    "# parameters\n",
    "TRAINING_ITERATIONS = 10\n",
    "TRAINING_FRACTION = 0.6\n",
    "\n",
    "# generate the data\n",
    "data = sc.parallelize(xrange(1,10001)) \\\n",
    "    .map(lambda x: LabeledPoint(random.random(), [random.random(), random.random(), random.random()]))\n",
    "\n",
    "# split the training and test sets\n",
    "splits = data.randomSplit([TRAINING_FRACTION, 1 - TRAINING_FRACTION], seed=42)\n",
    "training, test = (splits[0].cache(), splits[1])\n",
    "\n",
    "# train the model\n",
    "model = LinearRegressionWithSGD.train(training, TRAINING_ITERATIONS)\n",
    "\n",
    "# get r2 score\n",
    "predictions = test.map(lambda x: (float(model.predict(x.features)), x.label))\n",
    "print RegressionMetrics(predictions).r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML\n",
    "Spark ML implements the ideas of transformers, estimators, and pipelines by standardizing APIS across machine learning algorithms. This can streamline more complex workflows.\n",
    "\n",
    "The core functionality includes:\n",
    "* DataFrames - built off Spark SQL, can be created either directly or from RDDs as seen above\n",
    "* Transformers - algorithms that accept a DataFrame as input and return a DataFrame as output\n",
    "* Estimators - algorithms that accept a DataFrame as input and return a Transformer as output\n",
    "* Pipelines - chaining together Transformers and Estimators\n",
    "* Parameters - common API for specifying hyperparameters\n",
    "\n",
    "For example, a learning algorithm can be implemented as an Estimator which trains on a DataFrame of features and returns a Transformer which can output predictions based on a test DataFrame.\n",
    "\n",
    "Full documentation can be found [here](http://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(title=u'A decent guided tour of Spark and its major components.', label=0.0, prediction=1.0)\n",
      "Row(title=u'10/10 would buy again', label=1.0, prediction=1.0)\n",
      "Row(title=u'it is simple to follow. well organized. straight ...', label=1.0, prediction=1.0)\n",
      "Row(title=u'Just what you need to get started in Apache Spark.', label=1.0, prediction=1.0)\n",
      "Row(title=u'Very good book for learning Spark', label=1.0, prediction=1.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "reviews = [(\"Prose is well-written, but style is an impediment to learning. Should be called 'Reviewing Spark,' not 'Learning Spark'\", 0.0),\n",
    "            (\"Nice Headstart to Spark\", 1.0),\n",
    "            (\"Start here: Excellent reference for Spark\", 1.0),\n",
    "            (\"Insightful and so Spark-tastic!\", 1.0),\n",
    "            (\"Good intro but wordy and lacking details in areas\", 0.0),\n",
    "            (\"Best of the Books Currently Available\", 1.0),\n",
    "            (\"A good resource for people interested in learning Spark\", 1.0),\n",
    "            (\"Great Overview\", 1.0)]\n",
    "\n",
    "test_reviews = [(\"A decent guided tour of Spark and its major components.\", 0.0),\n",
    "                (\"10/10 would buy again\", 1.0),\n",
    "                (\"it is simple to follow. well organized. straight ...\", 1.0),\n",
    "                (\"Just what you need to get started in Apache Spark.\", 1.0),\n",
    "                (\"Very good book for learning Spark\", 1.0)]\n",
    "\n",
    "training = sqlContext.createDataFrame(reviews, [\"title\", \"label\"])\n",
    "test = sqlContext.createDataFrame(test_reviews, [\"title\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and logistic regression.\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "logreg = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, logreg])\n",
    "\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Make predictions on test documents\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"title\", \"label\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Use SVM to predict colon cancer from gene expressions\n",
    "You can start getting a feel for the MLlib operations by following the [Spark docs example](https://spark.apache.org/docs/1.3.0/mllib-linear-methods.html#linear-support-vector-machines-svms) on this dataset.\n",
    "\n",
    "#### About the data format: LibSVM\n",
    "MLlib conveniently provides a data loading method, `MLUtils.loadLibSVMFile()`, for the LibSVM format for which many other languages (R, Matlab, etc.) also have loading methods.  \n",
    "A dataset of *n* features will have one row per datum, with the label and values of each feature organized as follows:\n",
    ">{label} 1:{value} 2:{value} ... n:{value}\n",
    "\n",
    "Take these two datapoints with six features and labels of -1 and 1 respectively as an example:\n",
    ">-1.000000  1:2.080750 2:1.099070 3:0.927763 4:1.029080 5:-0.130763 6:1.265460  \n",
    "1.000000  1:1.109460 2:0.786453 3:0.445560 4:-0.146323 5:-0.996316 6:0.555759 \n",
    "\n",
    "#### About the colon-cancer dataset\n",
    "This dataset was introduced in the 1999 paper [Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays.](http://www.pnas.org/content/96/12/6745.short)  \n",
    "\n",
    "Here's the abstract of the paper:  \n",
    "> *Oligonucleotide arrays can provide a broad picture of the state of the cell, by monitoring the expression level of thousands of genes at the same time. It is of interest to develop techniques for extracting useful information from the resulting data sets. Here we report the application of a two-way clustering method for analyzing a data set consisting of the expression patterns of different cell types. Gene expression in 40 tumor and 22 normal colon tissue samples was analyzed with an Affymetrix oligonucleotide array complementary to more than 6,500 human genes. An efficient two-way clustering algorithm was applied to both the genes and the tissues, revealing broad coherent patterns that suggest a high degree of organization underlying gene expression in these tissues. Coregulated families of genes clustered together, as demonstrated for the ribosomal proteins. Clustering also separated cancerous from noncancerous tissue and cell lines from in vivo tissues on the basis of subtle distributed patterns of genes even when expression of individual genes varied only slightly between the tissues. Two-way clustering thus may be of use both in classifying genes into functional groups and in classifying tissues based on gene expression.*\n",
    "\n",
    "There are 2000 features, 62 data points (40 tumor (label=0), 22 normal (label=1)), and 2 classes (labels) for the colon cancer dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
