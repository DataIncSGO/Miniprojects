{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Technologies \n",
    "Streaming is useful for realtime analytics, online machine learning, continuous computation, and more.\n",
    "\n",
    "### Apache Kafka\n",
    "*A distributed, partitioned, replicated commit log service.*     \n",
    "\n",
    "Data streams are partitioned and spread over a cluster of machines to allow data streams larger than the capability of any single machine and to allow clusters of coordinated consumers\n",
    "![kafka](images/kafka_producer_consumer.png)\n",
    "\n",
    "- Kafka maintains feeds of messages in categories called topics.\n",
    "- Processes that publish messages to a Kafka topic: *producers.*\n",
    "- Processes that subscribe to topics and process the feed of published messages: *consumers*\n",
    "- Kafka is run as a cluster comprised of one or more servers, each of which is called a *broker.*\n",
    "- Java client for communication between clients and servers is provided natively; other languages are available.\n",
    "\n",
    "#### Useful for:\n",
    "- Website activity tracking\n",
    "- Metrics in operational monitoring, i.e. aggregating stats from distributed applications\n",
    "- Log aggregation, collecting the physical log files off servers and putting them in a central place (e.g. HDFS) for processing\n",
    "- Stream processing\n",
    "\n",
    "### Apache Storm\n",
    "*\"A free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing.\"*\n",
    "\n",
    "- Fast: can process 1 million+ tuples per second per node!\n",
    "- Scalable and fault-tolerant\n",
    "- Easy to set up and operate\n",
    "\n",
    "#### Abstractions \n",
    "- **Core abstraction:** The *stream*, an unbounded sequence of tuples. Storm provides the primitives for transforming a stream into a new stream in a distributed and reliable way. \n",
    "- **Spout:** a source of streams, e.g. a connection to the Twitter API\n",
    "- **Bolt:** consumes any number of input streams and runs processing logic; possibly emits new streams. \n",
    "    - E.g. run functions, filter, aggregate streams, join streams, talk to databases, etc.\n",
    "- Networks of spouts and bolts are packaged into a **topology**, which is the top level abstraction that you submit to Storm clusters for execution. \n",
    "![storm](images/storm_topology.png)\n",
    "\n",
    "- Each node in the topology executes in parallel (you can specify the level of parallelism for each node) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming\n",
    "\n",
    "**Core abstraction:** The *DStream*, aka discretized stream - a sequence of data arriving over time.\n",
    "- Internally represented as a sequence of RDDs arriving at each time step\n",
    "- Can be created from various input sources, e.g. Kafka, HDFS\n",
    "- Two types of operations: \n",
    "    - *Transformations* yield a new DStream. Many of the same operations available on RDDS, in addition to operations related to time e.g. sliding windows\n",
    "    - *Output operations* write data to an external system \n",
    "\n",
    "### To use Spark Streaming in an application\n",
    "#### Include as imports:\n",
    "```scala\n",
    "import org.apache.spark.streaming.StreamingContext\n",
    "import org.apache.spark.streaming.StreamingContext._\n",
    "import org.apache.spark.streaming.dstream.DStream\n",
    "import org.apache.spark.streaming.Duration\n",
    "import org.apache.spark.streaming.Seconds \n",
    "```\n",
    "\n",
    "#### Include in your `build.sbt`:\n",
    "```scala\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-streaming\" % \"1.3.0\" % \"provided\"\n",
    "```\n",
    "\n",
    "#### Building\n",
    "`sbt clean assembly` or `sbt assembly`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Tweets Demo\n",
    "\n",
    "The project lives here: `datacourse/projects/streaming_twitter_cl`\n",
    "\n",
    "#### Collecting tweets with built-in Twitter utilities\n",
    "```scala\n",
    " val tweetStream = TwitterUtils.createStream(ssc, Utils.getAuth)\n",
    "  .map(gson.toJson(_))\n",
    "\n",
    "tweetStream.foreachRDD((rdd, time) => {\n",
    "  val count = rdd.count()\n",
    "  if (count > 0) {\n",
    "    val outputRDD = rdd.repartition(partitionsEachInterval)\n",
    "    outputRDD.saveAsTextFile(\n",
    "      outputDirectory + \"/tweets_\" + time.milliseconds.toString)\n",
    "    numTweetsCollected += count\n",
    "    if (numTweetsCollected > numTweetsToCollect) {\n",
    "      System.exit(0)\n",
    "    }\n",
    "  }\n",
    "})\n",
    "```\n",
    "\n",
    "#### Examining the tweets and training a K-means clustering model\n",
    "\n",
    "The example uses SparkSQL to examine the data based on the tweets. SparkSQL can load JSON files and infer the schema based on the data. The commands you pass into SparkSQL to bring back to stdout will follow pretty standard SQL syntax.\n",
    "\n",
    "```scala\n",
    "sqlContext.sql(<command>).collect().foreach(println)\n",
    "```\n",
    "\n",
    "This clustering aims to identify clusters of tweets written in the same language. We do so by vectorizing hashed bigrams of characters within each tweet to recognize common sequences of characters in languages. Here `tf` is a `HashingTF` from `mllib.feature.HashingTF`\n",
    "\n",
    "```scala\n",
    "  def featurize(s: String): Vector = {\n",
    "    tf.transform(s.sliding(2).toSeq)\n",
    "  }\n",
    "```\n",
    "\n",
    "And here we train a KMeans model from MLlib:\n",
    "```scala\n",
    "val vectors = texts.map(Utils.featurize).cache()\n",
    "val model = KMeans.train(vectors, numClusters, numIterations)\n",
    "sc.makeRDD(model.clusterCenters, numClusters).\n",
    "    saveAsObjectFile(outputModelDir)\n",
    "```\n",
    "\n",
    "#### Realtime classification\n",
    "\n",
    "Finally, let's apply the model we've created in realtime! We'll:\n",
    "1. load a Spark Streaming Context\n",
    "1. create a Twitter DStream and grab just their `text` field\n",
    "1. load the trained KMeans model\n",
    "1. Choose the id of a language cluster we're interested in, and apply the model to all tweets, filtering only on that specificed cluster to see if the printed output is what we expect. \n",
    "\n",
    "```scala\n",
    "println(\"Initializing Streaming Spark Context...\")\n",
    "val conf = new SparkConf().setAppName(this.getClass.getSimpleName)\n",
    "val ssc = new StreamingContext(conf, Seconds(5))\n",
    "\n",
    "println(\"Initializing Twitter stream...\")\n",
    "val tweets = TwitterUtils.createStream(ssc, Utils.getAuth)\n",
    "val statuses = tweets.map(_.getText)\n",
    "\n",
    "println(\"Initializing the the KMeans model...\")\n",
    "val model = new KMeansModel(ssc.sparkContext.objectFile[Vector](\n",
    "    modelFile.toString).collect())\n",
    "\n",
    "val filteredTweets = statuses\n",
    "  .filter(t => model.predict(Utils.featurize(t)) == clusterNumber)\n",
    "filteredTweets.print()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An exercise for the reader\n",
    "\n",
    "The first streaming model to make it into MLlib is a streaming k-means. This means that the cluster estimations can be dynamically updated as new data arrive. The algorithm uses a generalization of the mini-batch k-means update rule. For each batch of data, we assign all points to their nearest cluster, compute new cluster centers, then update each cluster. \n",
    "\n",
    "In this example we trained our model \"in batch\" on a static store of data. Try writing another class in the `streaming_twitter_cl` project that instead initializes a new `StreamingKMeans` model with random centers and the number of languages you want to classify as your number of clusters, and dynamically updates on fresh stream of Twitter ata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
